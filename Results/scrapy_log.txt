2023-08-15 23:33:09 [scrapy.utils.log] INFO: Scrapy 2.10.0 started (bot: maininfo)
2023-08-15 23:33:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19043-SP0
2023-08-15 23:33:09 [scrapy.addons] INFO: Enabled addons:
[]
2023-08-15 23:33:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'maininfo',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'Results/scrapy_log.txt',
 'LOG_LEVEL': 'ERROR',
 'NEWSPIDER_MODULE': 'maininfo.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['maininfo.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-08-15 23:33:09 [asyncio] DEBUG: Using selector: SelectSelector
2023-08-15 23:33:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-08-15 23:33:09 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2023-08-15 23:33:09 [scrapy.extensions.telnet] INFO: Telnet Password: 8723853a1476b27b
2023-08-15 23:33:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-08-15 23:33:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-08-15 23:33:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-08-15 23:33:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-08-15 23:33:10 [scrapy.core.engine] INFO: Spider opened
2023-08-15 23:33:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:33:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-08-15 23:33:10 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-cruise.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-cruise.ru'))])
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booker-spb.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-cruises.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-cruises.ru'))])
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://booking-cruise.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artisticswimming.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artisticswimming.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artisticswimming.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artisticswimming.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artisticswimming.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artisticswimming.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booker-spb.ru> (referer: None)
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://booking-cruises.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artisticswimming.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artisticswimming.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-desk.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-desk.ru'))])
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://booking-desk.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artisticswimming.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artisticswimming.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-cruise.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-cruise.ru'))])
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistik.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-desk.ru> (failed 1 times): 500 Internal Server Error
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://booking-cruise.ru> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dog.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://artisticswimming.ru
details : DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://artisticswimming.ru
details : DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dog.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-cruises.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-cruises.ru'))])
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dog.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dog.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-dog.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: booking-dog.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-dog.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: booking-dog.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dog.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://booking-cruises.ru> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-desk.ru> (failed 2 times): 500 Internal Server Error
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dogs.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistik.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dogs.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dog.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dogs.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dogs.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-dogs.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-dogs.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "artistik55.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'artistik55.ru'))])
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dog.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dog.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dogs.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dogs.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://booker-spb.ru/contacts> from <GET https://booker-spb.ru/contacts.html>
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistina.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistik55.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://artistik55.ru>
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-desk.ru> (failed 3 times): 500 Internal Server Error
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-desk.ru> (failed 3 times): 500 Internal Server Error
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (500) <GET https://booking-desk.ru> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistina.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dogs.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dogs.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-cruise.ru
details : Ignoring non-200 response
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-cruise.ru
details : Ignoring non-200 response
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistina.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistina.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistina.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: artistina.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistina.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: artistina.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booker-spb.ru/contacts> (referer: https://booker-spb.ru)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistik.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistik.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistik.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistik.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistina.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistina.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistina.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistina.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-cruises.ru
details : Ignoring non-200 response
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-cruises.ru
details : Ignoring non-200 response
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-dog.ru
details : DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-dog.ru
details : DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistik.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://artistik55.ru
details : Forbidden by robots.txt
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://artistik55.ru
details : Forbidden by robots.txt
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-desk.ru
details : Ignoring non-200 response
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-desk.ru
details : Ignoring non-200 response
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-dogs.ru
details : DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://booking-dogs.ru
details : DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://artistina.ru
details : DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://artistina.ru
details : DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:33:10 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://booker-spb.ru/contacts.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistium.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistik.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistineer.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistik.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistik.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistidian.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "artistizm.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'artistizm.ru'))])
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://artistik.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [root] ERROR: Parsing error
 url : https://artistik.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistizm.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://artistizm.ru>
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistivanov.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistika-gym.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booking-dubrovka.ru/robots.txt> (referer: None)
2023-08-15 23:33:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistium.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistivanov.ru> (referer: None)
2023-08-15 23:33:11 [root] ERROR: Parsing error
 url : https://artistizm.ru
details : Forbidden by robots.txt
2023-08-15 23:33:11 [root] ERROR: Parsing error
 url : https://artistizm.ru
details : Forbidden by robots.txt
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booking-dubrovka.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistineer.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglish.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistika-gym.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglishacademy.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 207 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 212 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 258 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 266 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 267 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 269 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 275 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 276 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 287 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 288 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 290 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 300 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 305 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 307 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 308 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 314 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 319 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 323 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 325 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 326 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 331 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 334 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 335 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 343 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 345 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 349 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 350 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 351 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 352 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 353 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 354 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 361 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 362 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 365 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 367 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 371 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 375 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 379 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 380 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 389 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 405 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 406 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 415 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 432 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 887 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 888 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 889 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 890 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 891 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 892 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 893 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 899 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 900 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 904 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 905 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 906 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 907 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 908 without any user agent to enforce it on.
2023-08-15 23:33:11 [protego] DEBUG: Rule at line 909 without any user agent to enforce it on.
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglishvibes.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglish.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistidian.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistivanov.ru/contacts> (referer: https://artistivanov.ru)
2023-08-15 23:33:11 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.freni.ru/robots.txt> from <GET https://freni.ru/robots.txt>
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglishvibes.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://frenglish.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 241: invalid start byte
2023-08-15 23:33:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://frenglish.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 241: invalid start byte
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-ace.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.freni.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://freniarus.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-ace.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-aqua.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.freni.ru/> from <GET https://freni.ru>
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotel-dubrovka.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-aqua.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://freniarus.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.freni.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglishacademy.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotel-dubrovka.ru/kontakty.html> (referer: https://booking-dubrovka.ru)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-lift.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-hvac.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mini.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic-aqua.ru/kontakty-frenic-aqua.html>
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-hvac.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-ace.ru/kontakty-frenic-ace.html> (referer: https://frenic-ace.ru)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-lift.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mini.ru> (referer: None)
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic.ru/robots.txt> (referer: None)
2023-08-15 23:33:11 [root] ERROR: Parsing error
 url : https://frenic-aqua.ru/kontakty-frenic-aqua.html
details : Forbidden by robots.txt
2023-08-15 23:33:11 [root] ERROR: Parsing error
 url : https://frenic-aqua.ru/kontakty-frenic-aqua.html
details : Forbidden by robots.txt
2023-08-15 23:33:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.freni.ru/> (referer: None)
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://frenkbit.ru/robots.txt> (referer: None)
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-08-15 23:33:12 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-08-15 23:33:12 [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.freni.ru/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf2 in position 22: invalid continuation byte
2023-08-15 23:33:12 [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.freni.ru/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf2 in position 22: invalid continuation byte
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkelcars.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkbit.ru> (referer: None)
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkel.ru/robots.txt> (referer: None)
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic-hvac.ru/kontakty-frenic-hvac.html>
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic-lift.ru/kontakt.html>
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mini.ru/kontakty-fuji-electric-frenic-mini.html> (referer: https://frenic-mini.ru)
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkelcars.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkel.ru> (referer: None)
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenic-hvac.ru/kontakty-frenic-hvac.html
details : Forbidden by robots.txt
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenic-hvac.ru/kontakty-frenic-hvac.html
details : Forbidden by robots.txt
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenic-lift.ru/kontakt.html
details : Forbidden by robots.txt
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenic-lift.ru/kontakt.html
details : Forbidden by robots.txt
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkelcars.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkelcars.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkelcars.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkelcars.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkelceramics.ru/robots.txt> (referer: None)
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkelcars.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenki.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkelceramics.ru> (referer: None)
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-crm.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenki.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkelcars.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic.ru> (referer: None)
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenki.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenki.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenki.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenki.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkelcars.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkelcars.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkiee.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkiee.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkiee.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkiee.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkiee.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: frenkiee.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkiee.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: frenkiee.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkiee.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkiee.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenkelcars.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenkelcars.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkiee.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkiee.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic.ru/kontakty.html>
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mega.ru/robots.txt> (referer: None)
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenki.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mega.ru> (referer: None)
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenki.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenkiee.ru
details : DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenkiee.ru
details : DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenic.ru/kontakty.html
details : Forbidden by robots.txt
2023-08-15 23:33:12 [root] ERROR: Parsing error
 url : https://frenic.ru/kontakty.html
details : Forbidden by robots.txt
2023-08-15 23:33:12 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "frenkit.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'frenkit.ru'))])
2023-08-15 23:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit.ru/robots.txt> (referer: None)
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenki.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenki.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic-mega.ru/kontakty-frenic-mega.html>
2023-08-15 23:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistino.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:13 [root] ERROR: Parsing error
 url : https://frenki.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:13 [root] ERROR: Parsing error
 url : https://frenki.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:13 [root] ERROR: Parsing error
 url : https://frenic-mega.ru/kontakty-frenic-mega.html
details : Forbidden by robots.txt
2023-08-15 23:33:13 [root] ERROR: Parsing error
 url : https://frenic-mega.ru/kontakty-frenic-mega.html
details : Forbidden by robots.txt
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit.ru> (referer: None)
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenklakh.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenklakh.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenklakh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenklakh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenklakh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: frenklakh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenklakh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: frenklakh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistique.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenklakh.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvrn.ru/robots.txt> (referer: None)
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenklakh.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenklakh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenklakh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://frenkit.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 697: invalid continuation byte
2023-08-15 23:33:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://frenkit.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 697: invalid continuation byte
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvshira.ru/robots.txt> (referer: None)
2023-08-15 23:33:13 [root] ERROR: Parsing error
 url : https://frenklakh.ru
details : DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [root] ERROR: Parsing error
 url : https://frenklakh.ru
details : DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit-pro.ru/robots.txt> (referer: None)
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvshira.ru> (referer: None)
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.teremvvk.ru/robots.txt> from <GET https://teremvvk.ru/robots.txt>
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit-pro.ru> (referer: None)
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvrn.ru> (referer: None)
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremwood.ru/robots.txt> (referer: None)
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teremvvk.ru/robots.txt> (referer: None)
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.teremvvk.ru/> from <GET https://teremvvk.ru>
2023-08-15 23:33:13 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teremyg.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teremyg.ru'))])
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teremvvk.ru/robots.txt> (referer: None)
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://teremyg.ru/> from <GET https://teremyg.ru/robots.txt>
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremzaim.ru/robots.txt> (referer: None)
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://teremzaim.ru>
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teremvvk.ru/> (referer: None)
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremyg.ru/> (referer: None)
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 53 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 111 without any user agent to enforce it on.
2023-08-15 23:33:13 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-08-15 23:33:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremwood.ru> (referer: None)
2023-08-15 23:33:13 [root] ERROR: Parsing error
 url : https://teremzaim.ru
details : Forbidden by robots.txt
2023-08-15 23:33:13 [root] ERROR: Parsing error
 url : https://teremzaim.ru
details : Forbidden by robots.txt
2023-08-15 23:33:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremzdravia.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvrn.ru/index.php?route=information/contact> (referer: https://teremvrn.ru)
2023-08-15 23:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremyg.ru> (referer: None)
2023-08-15 23:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit-pro.ru/contacts> (referer: https://frenkit-pro.ru)
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremzdravia.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremzdravia.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremzdravia.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teremzdravia.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teremzdravia.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teren-control.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-crm.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkele.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremwood.ru/contact> (referer: https://teremwood.ru)
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremzdravia.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teren.ru/robots.txt> (referer: None)
2023-08-15 23:33:14 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teren-control.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremzdravia.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremzdravia.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremzdravia.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terena.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terena.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terena.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terena.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terena.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terena.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terena.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terena.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terena.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terena.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terena.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terena.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:14 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teren-control.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teren-control.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teren-control.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teren-control.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://teremzdravia.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://teremzdravia.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://terena.ru
details : DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://terena.ru
details : DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:33:15 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:33:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenberg.ru/robots.txt> (referer: None)
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teren-control.ru> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenberg.ru> (referer: None)
2023-08-15 23:33:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teren.ru> (referer: None)
2023-08-15 23:33:15 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terencegrech.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terencegrech.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terencegrech.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terencegrech.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terencegrech.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terencegrech.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terencegrech.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terencegrech.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teren-control.ru> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terencegrech.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terencegrech.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenceuniform.ru/robots.txt> (referer: None)
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terencegrech.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terencegrech.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenchin.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenchin.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenchin.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenchin.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenchin.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenchin.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenchin.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenchin.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenchin.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistino.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teren-control.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teren-control.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenchin.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenchin.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenchin.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://terencegrech.ru
details : DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://terencegrech.ru
details : DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:33:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistique.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://teren-control.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://teren-control.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://terenchin.ru
details : DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [root] ERROR: Parsing error
 url : https://terenchin.ru
details : DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:33:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenceuniform.ru> (referer: None)
2023-08-15 23:33:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terendina.ru/robots.txt> (referer: None)
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenergoprom.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenergoprom.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenergoprom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenergoprom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenergoprom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenergoprom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenergoprom.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenergoprom.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenergoprom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenergoprom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkele.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:16 [root] ERROR: Parsing error
 url : https://terenergoprom.ru
details : DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [root] ERROR: Parsing error
 url : https://terenergoprom.ru
details : DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-crm.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-crm.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-crm.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:16 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-crm.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terence.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenda.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistino.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistino.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistino.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistino.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistique.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistique.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistique.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistique.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:17 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:33:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terendina.ru> (referer: None)
2023-08-15 23:33:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenga.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenga.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengh.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengh.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terengh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terengh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terengh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terengh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengh.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengh.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenga.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenga.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenga.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenga.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [root] ERROR: Parsing error
 url : https://terengh.ru
details : DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [root] ERROR: Parsing error
 url : https://terengh.ru
details : DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:33:18 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenga.ru> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenga.ru> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenghi.ru/robots.txt> (referer: None)
2023-08-15 23:33:18 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenga.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenga.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenghi.ru> (referer: None)
2023-08-15 23:33:18 [root] ERROR: Parsing error
 url : https://terenga.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [root] ERROR: Parsing error
 url : https://terenga.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereni4ev.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereni4ev.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereni4ev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereni4ev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://tereni4ev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://tereni4ev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereni4ev.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereni4ev.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereni4ev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereni4ev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenik.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenik.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenik.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenik.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenik.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenik.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenik.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenik.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenik.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenik.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenik.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenik.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:18 [root] ERROR: Parsing error
 url : https://tereni4ev.ru
details : DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [root] ERROR: Parsing error
 url : https://tereni4ev.ru
details : DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-group.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-group.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkele.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkele.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkele.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkele.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-group.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-group.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov-group.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov-group.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov-group.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov-group.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-group.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereng.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://terenik.ru
details : DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://terenik.ru
details : DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-sa.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-sa.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-sa.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-sa.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov-sa.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov-sa.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-sa.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-group.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-sa.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-group.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-group.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-sa.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-sa.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-crm.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://yurov-group.ru
details : DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://yurov-group.ru
details : DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://yurov-sa.ru
details : DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://yurov-sa.ru
details : DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terence.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov2111.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov2111.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov2111.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov2111.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov2111.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov2111.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov2111.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov2111.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov2111.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov2111.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov2111.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov2111.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenda.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://yurov2111.ru
details : DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://yurov2111.ru
details : DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:33:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurova-beauty.ru/robots.txt> (referer: None)
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurova-beauty.ru> (referer: None)
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurova.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurova.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurova.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://yurov.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:19 [root] ERROR: Parsing error
 url : https://yurov.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistino.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaa.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaa.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaa.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaa.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovaa.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovaa.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovaa.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovaa.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistique.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaa.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurova.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaa.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaa.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaa.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:20 [root] ERROR: Parsing error
 url : https://yurovaa.ru
details : DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [root] ERROR: Parsing error
 url : https://yurovaa.ru
details : DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:33:20 [root] ERROR: Parsing error
 url : https://yurova.ru
details : DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:20 [root] ERROR: Parsing error
 url : https://yurova.ru
details : DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengi.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkele.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereng.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-crm.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terence.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terence.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terence.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terence.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovamaks.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovamaks.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovamaks.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovamaks.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovamaks.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovamaks.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovamaks.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovamaks.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovamaks.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovamaks.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovgroup.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovgroup.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovgroup.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovgroup.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovgroup.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovgroup.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovgroup.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [root] ERROR: Parsing error
 url : https://yurovamaks.ru
details : DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [root] ERROR: Parsing error
 url : https://yurovamaks.ru
details : DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovgroup.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovgroup.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovgroup.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovich.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovich.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovich.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovich.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovich.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovich.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovich.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovich.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovich.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovich.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovich.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovich.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [root] ERROR: Parsing error
 url : https://yurovgroup.ru
details : DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [root] ERROR: Parsing error
 url : https://yurovgroup.ru
details : DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:33:21 [root] ERROR: Parsing error
 url : https://yurovich.ru
details : DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [root] ERROR: Parsing error
 url : https://yurovich.ru
details : DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:33:21 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://yurovkurs.ru/robots.txt> (referer: None)
2023-08-15 23:33:21 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:33:21 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-08-15 23:33:21 [protego] DEBUG: Rule at line 5 without any user agent to enforce it on.
2023-08-15 23:33:21 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenda.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenda.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenda.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenda.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:21 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://yurovkurs.ru> (referer: None)
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnik.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnik.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnik.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnik.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovnik.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovnik.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovnik.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovnik.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnik.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:21 [root] ERROR: Parsing error
 url : https://yurovkurs.ru
details : Ignoring non-200 response
2023-08-15 23:33:21 [root] ERROR: Parsing error
 url : https://yurovkurs.ru
details : Ignoring non-200 response
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnikova.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnikova.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnikova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnikova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovnikova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovnikova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnikova.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnik.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnikova.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnik.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnik.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnikova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnikova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [root] ERROR: Parsing error
 url : https://yurovnik.ru
details : DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:22 [root] ERROR: Parsing error
 url : https://yurovnik.ru
details : DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:33:22 [root] ERROR: Parsing error
 url : https://yurovnikova.ru
details : DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [root] ERROR: Parsing error
 url : https://yurovnikova.ru
details : DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaceramics.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:22 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "yurovo.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'yurovo.ru'))])
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistino.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:22 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://yurovo.ru/robots.txt> (referer: None)
2023-08-15 23:33:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovo.ru> (referer: None)
2023-08-15 23:33:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistique.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengi.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://yurovskaya.ru/robots.txt> (referer: None)
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 97 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 105 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 108 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 118 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 132 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 145 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 146 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 151 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 181 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 182 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 189 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 192 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 193 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 200 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 201 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 204 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 205 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 208 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 210 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 215 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 266 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 281 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 307 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 316 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 329 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 331 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 334 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 335 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 347 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 388 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 389 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 397 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 402 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 403 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 404 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 405 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 406 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-08-15 23:33:23 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskih.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskih.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskih.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskih.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskih.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskih.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskih.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskih.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovskaya.ru> (referer: None)
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskih.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskikh.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskikh.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskikh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskikh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskikh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskikh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskikh.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskih.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskikh.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskih.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskih.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskikh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskikh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkele.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://yurovskih.ru
details : DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://yurovskih.ru
details : DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://yurovskikh.ru
details : DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://yurovskikh.ru
details : DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereng.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereng.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://tereng.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://tereng.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskiy.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskiy.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskiy.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskiy.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskiy.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskiy.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskiy.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskiy.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskiy.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskiy.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-crm.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-crm.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terence.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovskiy-kirill.ru/robots.txt> (referer: None)
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovstudio.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovstudio.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovstudio.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovstudio.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovstudio.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovstudio.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovstudio.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovstudio.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovsky.ru/robots.txt> (referer: None)
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovstudio.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovstudio.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://yurovskiy.ru
details : DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://yurovskiy.ru
details : DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://booking-crm.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://booking-crm.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovwed.ru/robots.txt> (referer: None)
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://yurovstudio.ru
details : DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [root] ERROR: Parsing error
 url : https://yurovstudio.ru
details : DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovwed.ru> (referer: None)
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovsky.ru> (referer: None)
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovwed.ru/contacts> (referer: https://yurovwed.ru)
2023-08-15 23:33:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenda.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovskiy-kirill.ru> (referer: None)
2023-08-15 23:33:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaisy.ru/robots.txt> (referer: None)
2023-08-15 23:33:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovsky.ru/kontakty/> (referer: https://yurovsky.ru)
2023-08-15 23:33:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zait-service.ru/robots.txt> (referer: None)
2023-08-15 23:33:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zait-service.ru> (referer: None)
2023-08-15 23:33:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaceramics.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistino.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistino.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistique.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistique.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:24 [root] ERROR: Parsing error
 url : https://artistino.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:24 [root] ERROR: Parsing error
 url : https://artistino.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:24 [root] ERROR: Parsing error
 url : https://artistique.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:24 [root] ERROR: Parsing error
 url : https://artistique.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaita.ru/robots.txt> (referer: None)
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengi.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengi.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terengi.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terengi.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaisy.ru> (referer: None)
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkele.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkele.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitbiz55.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitbiz55.ru'))])
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://zaitbiz55.ru/robots.txt> (referer: None)
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaita.ru> (referer: None)
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitbiz55.ru> (referer: None)
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereng.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [root] ERROR: Parsing error
 url : https://frenkele.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [root] ERROR: Parsing error
 url : https://frenkele.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitc.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitc.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitc.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitc.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitc.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitc.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitc.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitc.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitc.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitbudushcheye.ru/robots.txt> (failed 1 times): 503 Service Unavailable
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitc.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitc.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitc.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitbudushcheye.ru/robots.txt> (failed 2 times): 503 Service Unavailable
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terence.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitbudushcheye.ru/robots.txt> (failed 3 times): 503 Service Unavailable
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitbudushcheye.ru/robots.txt> (failed 3 times): 503 Service Unavailable
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://zaitbudushcheye.ru/robots.txt> (referer: None)
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-08-15 23:33:25 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitbudushcheye.ru> (failed 1 times): 503 Service Unavailable
2023-08-15 23:33:25 [root] ERROR: Parsing error
 url : https://zaitc.ru
details : DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [root] ERROR: Parsing error
 url : https://zaitc.ru
details : DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitbudushcheye.ru> (failed 2 times): 503 Service Unavailable
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitbudushcheye.ru> (failed 3 times): 503 Service Unavailable
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitbudushcheye.ru> (failed 3 times): 503 Service Unavailable
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://zaitbudushcheye.ru> (referer: None)
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurp.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcev911.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [root] ERROR: Parsing error
 url : https://zaitbudushcheye.ru
details : Ignoring non-200 response
2023-08-15 23:33:25 [root] ERROR: Parsing error
 url : https://zaitbudushcheye.ru
details : Ignoring non-200 response
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcev911.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcev911.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcev911.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcev911.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcev911.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcev911.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcev911.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcev911.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcev911.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [root] ERROR: Parsing error
 url : https://zaitcev911.ru
details : DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [root] ERROR: Parsing error
 url : https://zaitcev911.ru
details : DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaisy.ru/contacts/> (referer: https://zaisy.ru)
2023-08-15 23:33:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitceva-photo.ru/robots.txt> (referer: None)
2023-08-15 23:33:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitcev.ru/robots.txt> (referer: None)
2023-08-15 23:33:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitceva-photo.ru> (referer: None)
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenda.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitcev.ru> (referer: None)
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaceramics.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaceramics.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovaceramics.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovaceramics.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevpetr.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevpetr.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevpetr.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevpetr.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcevpetr.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcevpetr.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevpetr.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevpetr.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevpetr.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevpetr.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitev.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitev.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitev.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitev.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [root] ERROR: Parsing error
 url : https://zaitcevpetr.ru
details : DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [root] ERROR: Parsing error
 url : https://zaitcevpetr.ru
details : DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:33:26 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitfarm.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitfarm.ru'))])
2023-08-15 23:33:26 [root] ERROR: Parsing error
 url : https://zaitev.ru
details : DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [root] ERROR: Parsing error
 url : https://zaitev.ru
details : DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:33:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://zaitfarm.ru/robots.txt> (referer: None)
2023-08-15 23:33:26 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:33:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitfarm.ru> (referer: None)
2023-08-15 23:33:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengi.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereng.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terence.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terence.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:27 [root] ERROR: Parsing error
 url : https://terence.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:27 [root] ERROR: Parsing error
 url : https://terence.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurp.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevmedia.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenda.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenda.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:28 [root] ERROR: Parsing error
 url : https://terenda.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:28 [root] ERROR: Parsing error
 url : https://terenda.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitivaiti.ru/robots.txt> (referer: None)
2023-08-15 23:33:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaceramics.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://zaitivaiti.ru/login> from <GET https://zaitivaiti.ru>
2023-08-15 23:33:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://zaitivaiti.ru/cms/system/login> from <GET https://zaitivaiti.ru/login>
2023-08-15 23:33:28 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zaitivaiti.ru/cms/system/login>
2023-08-15 23:33:28 [root] ERROR: Parsing error
 url : https://zaitivaiti.ru/cms/system/login
details : Forbidden by robots.txt
2023-08-15 23:33:28 [root] ERROR: Parsing error
 url : https://zaitivaiti.ru/cms/system/login
details : Forbidden by robots.txt
2023-08-15 23:33:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitivit.ru/robots.txt> (referer: None)
2023-08-15 23:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitivit.ru> (referer: None)
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitochka.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaiti.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitochka.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitochka.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitochka.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitochka.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitochka.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitochka.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitochka.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitochka.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitochka.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitomart.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitomart.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitomart.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitomart.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitomart.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitomart.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitomart.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitomart.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitomart.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitomart.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitomart.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitomart.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [root] ERROR: Parsing error
 url : https://zaitochka.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [root] ERROR: Parsing error
 url : https://zaitochka.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:33:29 [root] ERROR: Parsing error
 url : https://zaitomart.ru
details : DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [root] ERROR: Parsing error
 url : https://zaitomart.ru
details : DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengi.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereng.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereng.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurp.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurp.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurp.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurp.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [root] ERROR: Parsing error
 url : https://tereng.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [root] ERROR: Parsing error
 url : https://tereng.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitova.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitova.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitova.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitova.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [root] ERROR: Parsing error
 url : https://zaitova.ru
details : DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [root] ERROR: Parsing error
 url : https://zaitova.ru
details : DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:33:30 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovaelena.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovaelena.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovaelena.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovaelena.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovaelena.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovaelena.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovaelena.ru> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovaelena.ru> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevmedia.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovaelena.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovaelena.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaceramics.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:30 [root] ERROR: Parsing error
 url : https://zaitovaelena.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:30 [root] ERROR: Parsing error
 url : https://zaitovaelena.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:33:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaiti.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitov.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengi.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengi.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurp.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:32 [root] ERROR: Parsing error
 url : https://terengi.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:32 [root] ERROR: Parsing error
 url : https://terengi.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovanazira.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevmedia.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevmedia.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcevmedia.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcevmedia.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovrr.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovrr.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovrr.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovrr.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovrr.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovrr.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovrr.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovrr.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovrr.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovrr.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [root] ERROR: Parsing error
 url : https://zaitovrr.ru
details : DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [root] ERROR: Parsing error
 url : https://zaitovrr.ru
details : DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:33:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitovstudio.ru/robots.txt> (referer: None)
2023-08-15 23:33:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitovstudio.ru> (referer: None)
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitov.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitoys.ru/robots.txt> (referer: None)
2023-08-15 23:33:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitoys.ru> (referer: None)
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaits.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaits.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaits.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaits.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaits.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaits.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaits.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaits.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaits.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaits.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaits.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaits.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaiti.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaiti.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaiti.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaiti.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsburg.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsburg.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsburg.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsburg.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsburg.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsburg.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsburg.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaits.ru
details : DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaits.ru
details : DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.petrzaytsev.ru> from <GET https://zaitseff.ru/robots.txt>
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaceramics.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaceramics.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsburg.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsburg.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsburg.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurp.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-1.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-1.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-1.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-1.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-1.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-1.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-1.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-1.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-1.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-1.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://yurovaceramics.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://yurovaceramics.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsburg.ru
details : DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsburg.ru
details : DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-1.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-1.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:33:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.petrzaytsev.ru> (referer: None)
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:33:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitseff.ru> (referer: None)
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-done.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-done.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-done.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-done.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-done.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-done.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-done.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-done.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-done.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-done.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://zaitsev-consulting.ru/robots.txt> (referer: None)
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 5 without any user agent to enforce it on.
2023-08-15 23:33:34 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-done.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-done.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:33:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-drum.ru/robots.txt> (referer: None)
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-life.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-life.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-life.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-life.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-life.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-life.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-life.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-life.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-life.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-life.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-drum.ru> (referer: None)
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-mail.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-mail.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-mail.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-mail.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-mail.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-mail.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-mail.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-nsk.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-nsk.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-nsk.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-nsk.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-nsk.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-nsk.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-nsk.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-mail.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-nsk.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-mail.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-mail.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-nsk.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-nsk.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-life.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-life.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-mail.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-mail.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-nsk.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:34 [root] ERROR: Parsing error
 url : https://zaitsev-nsk.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:33:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-prosluh.ru/robots.txt> (referer: None)
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zaitsev-prosluh.ru>
2023-08-15 23:33:35 [root] ERROR: Parsing error
 url : https://zaitsev-prosluh.ru
details : Forbidden by robots.txt
2023-08-15 23:33:35 [root] ERROR: Parsing error
 url : https://zaitsev-prosluh.ru
details : Forbidden by robots.txt
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovanazira.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-seo.ru/robots.txt> (referer: None)
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-top.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-top.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-top.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-top.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-top.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-top.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-top.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-top.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-top.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-top.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroclinic.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroclinic.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevmedia.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroclinic.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroclinic.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroclinic.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroclinic.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroclinic.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [root] ERROR: Parsing error
 url : https://zaitsev-top.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [root] ERROR: Parsing error
 url : https://zaitsev-top.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroclinic.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroclinic.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroclinic.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [root] ERROR: Parsing error
 url : https://zaitsev.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [root] ERROR: Parsing error
 url : https://zaitsev.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:33:35 [root] ERROR: Parsing error
 url : https://zveroclinic.ru
details : DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [root] ERROR: Parsing error
 url : https://zveroclinic.ru
details : DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:33:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroclub.ru/robots.txt> (referer: None)
2023-08-15 23:33:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-consulting.ru> (referer: None)
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitov.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitov.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitov.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitov.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverocomp.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverocomp.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverocomp.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverocomp.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverocomp.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverocomp.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverocomp.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverocomp.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverocomp.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaiti.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurp.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurp.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverocomp.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverocomp.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverocomp.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zverodel.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zverodel.ru'))])
2023-08-15 23:33:36 [root] ERROR: Parsing error
 url : https://yurp.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [root] ERROR: Parsing error
 url : https://yurp.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://zverodel.ru/robots.txt> from <GET https://zverodel.ru/robots.txt>
2023-08-15 23:33:36 [root] ERROR: Parsing error
 url : https://zverocomp.ru
details : DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [root] ERROR: Parsing error
 url : https://zverocomp.ru
details : DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:33:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://zverodel.ru/robots.txt> (referer: None)
2023-08-15 23:33:36 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://zverodel.ru/> from <GET https://zverodel.ru>
2023-08-15 23:33:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://zverodel.ru/> (referer: None)
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodom.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodom.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverodom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverodom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodom.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodom.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodrom.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodrom.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodrom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodrom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodrom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverodrom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodrom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverodrom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodrom.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodrom.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodrom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodrom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [root] ERROR: Parsing error
 url : https://zverodom.ru
details : DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [root] ERROR: Parsing error
 url : https://zverodom.ru
details : DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:33:37 [root] ERROR: Parsing error
 url : https://zverodrom.ru
details : DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [root] ERROR: Parsing error
 url : https://zverodrom.ru
details : DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:33:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroferm.ru/robots.txt> (referer: None)
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovanazira.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovanazira.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovanazira.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovanazira.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroferma.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroferma.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroferma.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroferma.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroferma.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zveroferma.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroferma.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zveroferma.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroferma.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroferma.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroferma.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroferma.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroferm.ru> (referer: None)
2023-08-15 23:33:37 [root] ERROR: Parsing error
 url : https://zveroferma.ru
details : DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [root] ERROR: Parsing error
 url : https://zveroferma.ru
details : DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:33:37 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zveroff.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zveroff.ru'))])
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroff.ru/robots.txt> (failed 1 times): 502 Bad Gateway
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroff.ru/robots.txt> (failed 2 times): 502 Bad Gateway
2023-08-15 23:33:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevmedia.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroclub.ru> (referer: None)
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroff.ru/robots.txt> (failed 3 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroff.ru/robots.txt> (failed 3 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.core.engine] DEBUG: Crawled (502) <GET https://zveroff.ru/robots.txt> (referer: None)
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 53 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 97 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 103 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 112 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 118 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 119 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 123 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 151 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-08-15 23:33:38 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitov.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroff.ru> (failed 1 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverogorod.ru/robots.txt> (referer: None)
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodar.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverogorod.ru> (failed 1 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverograd.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-seo.ru> (referer: None)
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverograd.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverogorod.ru> (failed 2 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverogorod.ru> (failed 3 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverogorod.ru> (failed 3 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.core.engine] DEBUG: Crawled (502) <GET https://zverogorod.ru> (referer: None)
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverograd.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverograd.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverograd.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverograd.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [root] ERROR: Parsing error
 url : https://zverogorod.ru
details : Ignoring non-200 response
2023-08-15 23:33:38 [root] ERROR: Parsing error
 url : https://zverogorod.ru
details : Ignoring non-200 response
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroff.ru> (failed 2 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverograd.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverograd.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaiti.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverograd.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverograd.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://zverojaschery.ru/robots.txt> from <GET https://zverojaschery.ru/robots.txt>
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroff.ru> (failed 3 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroff.ru> (failed 3 times): 502 Bad Gateway
2023-08-15 23:33:38 [scrapy.core.engine] DEBUG: Crawled (502) <GET https://zveroff.ru> (referer: None)
2023-08-15 23:33:38 [root] ERROR: Parsing error
 url : https://zverograd.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [root] ERROR: Parsing error
 url : https://zverograd.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:33:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://zverojaschery.ru/robots.txt> (referer: None)
2023-08-15 23:33:38 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zverojaschery.ru>
2023-08-15 23:33:38 [root] ERROR: Parsing error
 url : https://zveroff.ru
details : Ignoring non-200 response
2023-08-15 23:33:38 [root] ERROR: Parsing error
 url : https://zveroff.ru
details : Ignoring non-200 response
2023-08-15 23:33:39 [root] ERROR: Parsing error
 url : https://zverojaschery.ru
details : Forbidden by robots.txt
2023-08-15 23:33:39 [root] ERROR: Parsing error
 url : https://zverojaschery.ru
details : Forbidden by robots.txt
2023-08-15 23:33:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-seo.ru/kontakty/> (referer: https://zaitsev-seo.ru)
2023-08-15 23:33:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovanazira.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevmedia.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevmedia.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitov.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:40 [root] ERROR: Parsing error
 url : https://zaitcevmedia.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:40 [root] ERROR: Parsing error
 url : https://zaitcevmedia.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodar.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverokon.ru/robots.txt> (referer: None)
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 112 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-08-15 23:33:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverokon.ru> (referer: None)
2023-08-15 23:33:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://zverokorm.ru/robots.txt> (referer: None)
2023-08-15 23:33:40 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:33:40 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://zverokorm.ru> (referer: None)
2023-08-15 23:33:41 [root] ERROR: Parsing error
 url : https://zverokorm.ru
details : Ignoring non-200 response
2023-08-15 23:33:41 [root] ERROR: Parsing error
 url : https://zverokorm.ru
details : Ignoring non-200 response
2023-08-15 23:33:41 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zverokot.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zverokot.ru'))])
2023-08-15 23:33:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaiti.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaiti.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:41 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://zverokot.ru> from <GET https://zverokot.ru/robots.txt>
2023-08-15 23:33:41 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://rf.ru/zverokot.ru> from <GET http://zverokot.ru>
2023-08-15 23:33:41 [root] ERROR: Parsing error
 url : https://zaiti.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:41 [root] ERROR: Parsing error
 url : https://zaiti.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroklub.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rf.ru/zverokot.ru> (referer: None)
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 5 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:33:41 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-08-15 23:33:41 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://zverokot.ru> from <GET https://zverokot.ru>
2023-08-15 23:33:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovanazira.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:41 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://rf.ru/zverokot.ru> from <GET http://zverokot.ru>
2023-08-15 23:33:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rf.ru/robots.txt> (referer: None)
2023-08-15 23:33:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rf.ru/zverokot.ru> (referer: None)
2023-08-15 23:33:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodar.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodar.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:42 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodar.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:42 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodar.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitov.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitov.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:42 [root] ERROR: Parsing error
 url : https://zaitov.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:42 [root] ERROR: Parsing error
 url : https://zaitov.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroklub.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovanazira.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovanazira.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:44 [root] ERROR: Parsing error
 url : https://zaitovanazira.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:44 [root] ERROR: Parsing error
 url : https://zaitovanazira.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolavka.ru/robots.txt> (referer: None)
2023-08-15 23:33:44 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zverolavka.ru>
2023-08-15 23:33:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolash.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodar.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:44 [root] ERROR: Parsing error
 url : https://zverolavka.ru
details : Forbidden by robots.txt
2023-08-15 23:33:44 [root] ERROR: Parsing error
 url : https://zverolavka.ru
details : Forbidden by robots.txt
2023-08-15 23:33:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolog.ru/robots.txt> (referer: None)
2023-08-15 23:33:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolog.ru> (referer: None)
2023-08-15 23:33:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zait.ru/robots.txt> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:33:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroklub.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroklub.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:45 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroklub.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:45 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroklub.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolov.ru/robots.txt> (referer: None)
2023-08-15 23:33:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolog.ru/kontakty/> (referer: https://zverolog.ru)
2023-08-15 23:33:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolov.ru> (referer: None)
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolash.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitceva.ru/robots.txt> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodar.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolub.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolub.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolub.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolub.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolub.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverolub.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolub.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverolub.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolub.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroklub.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolyub.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolyub.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolyub.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolyub.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolyub.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverolyub.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolyub.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverolyub.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolyub.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolub.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolyub.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolub.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolub.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolyub.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolyub.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [root] ERROR: Parsing error
 url : https://zverolub.ru
details : DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:48 [root] ERROR: Parsing error
 url : https://zverolub.ru
details : DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:33:48 [root] ERROR: Parsing error
 url : https://zverolyub.ru
details : DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [root] ERROR: Parsing error
 url : https://zverolyub.ru
details : DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:33:48 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zveromag.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zveromag.ru'))])
2023-08-15 23:33:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveromag.ru/robots.txt> (referer: None)
2023-08-15 23:33:48 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zveromag.ru>
2023-08-15 23:33:49 [root] ERROR: Parsing error
 url : https://zveromag.ru
details : Forbidden by robots.txt
2023-08-15 23:33:49 [root] ERROR: Parsing error
 url : https://zveromag.ru
details : Forbidden by robots.txt
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolash.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolash.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolash.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolash.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto> from <GET https://ecsto.ru/robots.txt>
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodar.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodar.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto> (referer: None)
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 109 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 111 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 119 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 137 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 159 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 161 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 165 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 167 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 169 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 171 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 173 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 176 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 177 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 185 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 189 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 201 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 204 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 208 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 210 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 211 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 225 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 226 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 232 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 233 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 236 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 238 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 239 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 243 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 244 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 245 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 247 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 252 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 256 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 267 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 270 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 276 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 278 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 279 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 288 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 290 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 293 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 294 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 296 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 300 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 302 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 312 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 321 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 326 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 329 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 331 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 334 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 335 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 340 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 341 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 344 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 345 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 349 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 351 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 354 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 357 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 360 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 363 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 366 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 369 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 370 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 371 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 372 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 389 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 402 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 406 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 414 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 415 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 417 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 418 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 419 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 420 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 421 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 422 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 424 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 426 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 433 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 434 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 435 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 436 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 437 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 438 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 439 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 440 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 441 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 442 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 443 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 444 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 445 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 446 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 450 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 453 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 454 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 455 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 458 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 461 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 466 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 473 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 474 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 475 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 478 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 485 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 487 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 490 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 493 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 496 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 499 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 504 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 514 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 517 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 520 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 521 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 524 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 527 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 530 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 531 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 534 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 535 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 536 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 539 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 542 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 545 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 549 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 550 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 552 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 555 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 560 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 565 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 566 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 567 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 568 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 569 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 571 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 574 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 575 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 579 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 580 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 581 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 582 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 583 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 584 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 585 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 588 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 589 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 592 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 593 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 595 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 596 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 597 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 598 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 601 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 602 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 603 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 604 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 605 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 606 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 607 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 611 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 613 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 616 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 618 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 622 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 623 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 625 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 627 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 631 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 632 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 633 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 634 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 635 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 636 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 637 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 642 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 643 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 644 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 645 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 648 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 651 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 654 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 655 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 658 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 659 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 660 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 663 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 666 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 670 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 671 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 672 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 674 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 676 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 679 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 681 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 682 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 685 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 686 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 689 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 690 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 691 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 692 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 695 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 699 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 700 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 701 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 704 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 705 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 706 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 707 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 708 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 710 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 713 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 714 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 715 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 716 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 717 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 718 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 719 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 722 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 724 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 725 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 726 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 727 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 728 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 729 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 730 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 736 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 737 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 738 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 739 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 740 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 741 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 745 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 746 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 753 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 765 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 768 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 769 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 770 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 771 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 782 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 788 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 801 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 803 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 837 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 848 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 849 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 852 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 853 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 856 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 857 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 858 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 883 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 886 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 905 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 906 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 907 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 908 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 925 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 969 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 987 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1024 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1071 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1080 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1136 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1140 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1148 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1154 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1155 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1158 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1159 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1169 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1183 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1190 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1193 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1197 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1210 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1253 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1265 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1295 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1335 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1343 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1399 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1403 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1411 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1412 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1415 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1416 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1419 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1438 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1441 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1460 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1461 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1462 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1463 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1468 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1495 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1516 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1531 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1548 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1576 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1577 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1621 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1637 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1638 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1672 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1673 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1718 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1719 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1728 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1786 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1787 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1792 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1795 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1805 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1814 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1818 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1829 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1830 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1833 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1834 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1843 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1846 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1890 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1891 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1893 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1894 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1896 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1897 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1920 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1937 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1940 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1943 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1967 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 1973 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 6107 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 6126 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 6181 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 6197 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 6259 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 6275 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 10413 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 10432 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11069 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11071 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11087 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11099 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11117 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11130 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11142 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11153 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11165 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11177 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11217 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11218 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11219 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11220 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11221 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11222 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11223 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11224 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11225 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11226 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11227 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11228 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11229 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11236 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11249 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11250 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11251 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11252 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11253 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11254 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11255 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11256 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11257 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11258 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11259 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11260 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11261 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11278 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11299 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11313 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11365 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11381 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11393 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11432 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11451 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11464 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11469 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11470 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11504 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11523 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11532 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11533 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11534 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11537 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11540 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11541 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11554 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11557 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11588 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11656 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11660 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11665 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11682 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11697 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11712 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11727 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11742 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11757 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11772 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11787 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11814 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11872 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11880 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11898 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11914 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11930 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11936 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11937 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11940 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11966 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 11991 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12000 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12001 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12002 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12007 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12011 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12012 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12016 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12017 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12018 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12019 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12020 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12024 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12028 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12029 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12030 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12031 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12033 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12034 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12035 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12036 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12037 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12041 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12042 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12043 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12044 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12046 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12047 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12048 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12049 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12050 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12051 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12052 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12053 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12057 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12058 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12059 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12060 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12061 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12062 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12063 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12064 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12065 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12066 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12067 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12071 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12072 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12073 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12074 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12075 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12076 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12077 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12078 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12079 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12080 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12081 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12084 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12085 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12088 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12089 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12092 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12093 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12097 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12099 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12101 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12102 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12103 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12104 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12105 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12106 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12107 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12108 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12109 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12110 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12111 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12125 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12137 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12148 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12160 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12173 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12177 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12229 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12247 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12252 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12265 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12283 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12287 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12289 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12296 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12300 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12313 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12316 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12319 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12330 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12331 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12332 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12333 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12334 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12335 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12336 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12337 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12338 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12340 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12341 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12342 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12351 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12352 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12353 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12354 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12388 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12426 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12427 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12428 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12429 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12480 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12516 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12529 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12571 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12595 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12598 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12599 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12600 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12601 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12612 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12613 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12631 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12660 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12671 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12713 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12714 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12724 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12725 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12728 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12739 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12804 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12806 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12814 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12821 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12851 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12866 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12867 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12868 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12875 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12911 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12947 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 12960 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13002 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13026 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13029 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13030 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13031 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13032 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13047 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13076 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13087 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13129 without any user agent to enforce it on.
2023-08-15 23:33:49 [protego] DEBUG: Rule at line 13130 without any user agent to enforce it on.
2023-08-15 23:33:49 [root] ERROR: Parsing error
 url : https://zverodar.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:49 [root] ERROR: Parsing error
 url : https://zverodar.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto> from <GET https://ecsto.ru>
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://ecstom-allon4.tmp.sinergium.ru/robots.txt> from <GET https://ecstom-allon4.ru/robots.txt>
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverok.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverok.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverok.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverok.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverok.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverok.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverok.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverok.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverok.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.emcmos.ru/robots.txt> (referer: None)
2023-08-15 23:33:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverok.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverok.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverok.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroklub.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ecstom-allon4.tmp.sinergium.ru/robots.txt> (referer: None)
2023-08-15 23:33:50 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://ecstom-allon4.ru>
2023-08-15 23:33:50 [root] ERROR: Parsing error
 url : https://zverok.ru
details : DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:50 [root] ERROR: Parsing error
 url : https://zverok.ru
details : DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:33:50 [root] ERROR: Parsing error
 url : https://ecstom-allon4.ru
details : Forbidden by robots.txt
2023-08-15 23:33:50 [root] ERROR: Parsing error
 url : https://ecstom-allon4.ru
details : Forbidden by robots.txt
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstom.ru/robots.txt> (referer: None)
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto> (referer: None)
2023-08-15 23:33:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://eacportal.com/sertifikat-tr-eaehs-ts/robots.txt> from <GET https://ecstr-certificate.ru/robots.txt>
2023-08-15 23:33:50 [root] ERROR: Parsing error
 url : https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto
details : Ignoring non-200 response
2023-08-15 23:33:50 [root] ERROR: Parsing error
 url : https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto
details : Ignoring non-200 response
2023-08-15 23:33:50 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ecstract.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ecstract.ru'))])
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstom.ru> (referer: None)
2023-08-15 23:33:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://ecstract.ru/robots.txt> from <GET https://ecstract.ru/robots.txt>
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ecstract.ru/robots.txt> (referer: None)
2023-08-15 23:33:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://ecstract.ru/> from <GET https://ecstract.ru>
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstrm.ru/robots.txt> (referer: None)
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://eacportal.com/sertifikat-tr-eaehs-ts/robots.txt> (referer: None)
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 59 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 218 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 225 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 251 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 258 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 263 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 264 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 266 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 267 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 284 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 326 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 336 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 424 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 432 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 530 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 531 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 542 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 565 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 572 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 580 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 590 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 623 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 644 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 645 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 660 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 662 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 666 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 700 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 761 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 770 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 782 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 792 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 803 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 814 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 815 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 821 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 822 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 824 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 825 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 827 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 828 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 830 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 831 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 845 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 846 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 847 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 848 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 849 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 851 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 852 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 853 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 855 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 856 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 857 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 859 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 860 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 862 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 863 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 864 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 865 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 867 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 868 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 869 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 871 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 872 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 873 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 874 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 875 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 876 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 877 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 878 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 879 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 880 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 881 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 882 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 883 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 884 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 885 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 886 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 887 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 888 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 889 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 890 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 891 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 892 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 893 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 894 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 895 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 896 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 897 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 898 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 899 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 900 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 901 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 902 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 903 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 904 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 911 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 944 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 962 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 973 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 976 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 981 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 982 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 983 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 989 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1002 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1005 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1006 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1007 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1008 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1009 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1012 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1014 without any user agent to enforce it on.
2023-08-15 23:33:50 [protego] DEBUG: Rule at line 1016 without any user agent to enforce it on.
2023-08-15 23:33:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstrm.ru> (referer: None)
2023-08-15 23:33:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://eacportal.com/sertifikat-tr-eaehs-ts/> from <GET https://ecstr-certificate.ru>
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstom.ru/contact-us/> (referer: https://ecstom.ru)
2023-08-15 23:33:51 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ecstro.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ecstro.ru'))])
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://eacportal.com/robots.txt> (referer: None)
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://ecstro.ru/robots.txt> (referer: None)
2023-08-15 23:33:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 436: invalid continuation byte
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstro.ru> (referer: None)
2023-08-15 23:33:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecstom.ru/contact-us/> (referer: https://ecstom.ru)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 90, in parse
    await csvwriter.fill_csv(response.url, title, description, emails, phones, postal_codes, inns, ogrns)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\CSVwriter.py", line 27, in fill_csv
    await writer.writerow(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\aiofiles\threadpool\utils.py", line 43, in method
    return await self._loop.run_in_executor(self._executor, cb)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\encodings\cp1251.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f9b7' in position 39: character maps to <undefined>
2023-08-15 23:33:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecstom.ru/contact-us/> (referer: https://ecstom.ru)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 90, in parse
    await csvwriter.fill_csv(response.url, title, description, emails, phones, postal_codes, inns, ogrns)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\CSVwriter.py", line 27, in fill_csv
    await writer.writerow(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\aiofiles\threadpool\utils.py", line 43, in method
    return await self._loop.run_in_executor(self._executor, cb)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\encodings\cp1251.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f9b7' in position 39: character maps to <undefined>
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://eacportal.com/sertifikat-tr-eaehs-ts/> (referer: None)
2023-08-15 23:33:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolash.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://ecstrm.ru/contact-us/> from <GET http://ecstrm.ru/contact-us/>
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstrm.ru/contact-us/> (referer: None)
2023-08-15 23:33:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveromania.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://eacportal.com/sertifikat-tr-eaehs-ts/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 90, in parse
    await csvwriter.fill_csv(response.url, title, description, emails, phones, postal_codes, inns, ogrns)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\CSVwriter.py", line 27, in fill_csv
    await writer.writerow(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\aiofiles\threadpool\utils.py", line 43, in method
    return await self._loop.run_in_executor(self._executor, cb)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\encodings\cp1251.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 106-107: character maps to <undefined>
2023-08-15 23:33:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://eacportal.com/sertifikat-tr-eaehs-ts/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 90, in parse
    await csvwriter.fill_csv(response.url, title, description, emails, phones, postal_codes, inns, ogrns)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\CSVwriter.py", line 27, in fill_csv
    await writer.writerow(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\aiofiles\threadpool\utils.py", line 43, in method
    return await self._loop.run_in_executor(self._executor, cb)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\encodings\cp1251.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 106-107: character maps to <undefined>
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstro.ru/contacts.html> (referer: https://ecstro.ru)
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstudent.ru/robots.txt> (referer: None)
2023-08-15 23:33:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstudent.ru> (referer: None)
2023-08-15 23:33:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecstudent.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 12514: invalid continuation byte
2023-08-15 23:33:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecstudent.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 12514: invalid continuation byte
2023-08-15 23:33:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroklub.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroklub.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:52 [root] ERROR: Parsing error
 url : https://zveroklub.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:52 [root] ERROR: Parsing error
 url : https://zveroklub.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstroy.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolash.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ecstract.ru/> (referer: None)
2023-08-15 23:33:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveromania.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstudio.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstroy.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolash.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolash.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:55 [root] ERROR: Parsing error
 url : https://zverolash.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:55 [root] ERROR: Parsing error
 url : https://zverolash.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsurgut.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveromania.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveromania.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:56 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveromania.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:56 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveromania.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstudio.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ecstract.ru/contacts.html> (referer: http://ecstract.ru/)
2023-08-15 23:33:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstroy.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstroy.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:57 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecstroy.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:57 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecstroy.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsurgut.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstudio.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstudio.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecstudio.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecstudio.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveromania.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv24.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv24.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv24.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv24.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsv24.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ecsv24.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsv24.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ecsv24.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv24.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv24.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv24.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv24.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [root] ERROR: Parsing error
 url : https://ecsv24.ru
details : DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:58 [root] ERROR: Parsing error
 url : https://ecsv24.ru
details : DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:33:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:33:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstroy.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsurgut.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsurgut.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsurgut.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsurgut.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstudio.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveromania.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsy.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstroy.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroland.ru/robots.txt> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsurgut.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstudio.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveromania.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveromania.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsys.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsys.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsys.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsys.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsys.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ecsys.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsys.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ecsys.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsys.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsy.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsys.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsys.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsys.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [root] ERROR: Parsing error
 url : https://zveromania.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [root] ERROR: Parsing error
 url : https://zveromania.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [root] ERROR: Parsing error
 url : https://ecsys.ru
details : DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [root] ERROR: Parsing error
 url : https://ecsys.ru
details : DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsv.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsv.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:03 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://ecsyst.ru/robots.txt> (referer: None)
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-08-15 23:34:03 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:34:04 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://ecsyst.ru> (referer: None)
2023-08-15 23:34:04 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ecsystem.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ecsystem.ru'))])
2023-08-15 23:34:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://ecsystem.ru/robots.txt> (referer: None)
2023-08-15 23:34:04 [root] ERROR: Parsing error
 url : https://ecsyst.ru
details : Ignoring non-200 response
2023-08-15 23:34:04 [root] ERROR: Parsing error
 url : https://ecsyst.ru
details : Ignoring non-200 response
2023-08-15 23:34:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecsystem.ru> (referer: None)
2023-08-15 23:34:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstroy.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstroy.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:04 [root] ERROR: Parsing error
 url : https://ecstroy.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:04 [root] ERROR: Parsing error
 url : https://ecstroy.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsurgut.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecsytech.ru/robots.txt> (referer: None)
2023-08-15 23:34:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstudio.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstudio.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:05 [root] ERROR: Parsing error
 url : https://ecstudio.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:05 [root] ERROR: Parsing error
 url : https://ecstudio.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecszhbi.ru/robots.txt> (referer: None)
2023-08-15 23:34:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecsytech.ru> (referer: None)
2023-08-15 23:34:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecsytech.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdd in position 105: invalid continuation byte
2023-08-15 23:34:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecsytech.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdd in position 105: invalid continuation byte
2023-08-15 23:34:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-auto.ru/robots.txt> (referer: None)
2023-08-15 23:34:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-auto.ru> (referer: None)
2023-08-15 23:34:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-service.ru/robots.txt> (referer: None)
2023-08-15 23:34:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsy.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsy.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:05 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsy.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:05 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsy.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-auto.ru/index.phtml?id=6> (referer: https://ect-auto.ru)
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-service.ru> (referer: None)
2023-08-15 23:34:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-shop.ru/robots.txt> (referer: None)
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://ect-shop.ru>
2023-08-15 23:34:06 [root] ERROR: Parsing error
 url : https://ect-shop.ru
details : Forbidden by robots.txt
2023-08-15 23:34:06 [root] ERROR: Parsing error
 url : https://ect-shop.ru
details : Forbidden by robots.txt
2023-08-15 23:34:06 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ect-telecoms.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ect-telecoms.ru'))])
2023-08-15 23:34:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-telecoms.ru/robots.txt> (referer: None)
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-telecoms.ru> (failed 1 times): 500 Internal Server Error
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsystems.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecszhbi.ru> (referer: None)
2023-08-15 23:34:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-service.ru/ru/contact-us> (referer: https://ect-service.ru)
2023-08-15 23:34:06 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ect-telecoms.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ect-telecoms.ru'))])
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zait.ru/robots.txt> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:06 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ect-travel.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ect-travel.ru'))])
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-telecoms.ru> (failed 2 times): 500 Internal Server Error
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://ect-travel.ru/robots.txt> from <GET https://ect-travel.ru/robots.txt>
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsurgut.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsurgut.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ect-travel.ru/robots.txt> (referer: None)
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://ect-travel.ru/> from <GET https://ect-travel.ru>
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-truck.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-truck.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-truck.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-truck.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ect-truck.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ect-truck.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:06 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ect-truck.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ect-truck.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-truck.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-truck.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-truck.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-truck.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:07 [root] ERROR: Parsing error
 url : https://ecsurgut.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:07 [root] ERROR: Parsing error
 url : https://ecsurgut.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecszhbi.ru/contacts/> (referer: https://ecszhbi.ru)
2023-08-15 23:34:07 [root] ERROR: Parsing error
 url : https://ect-truck.ru
details : DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:07 [root] ERROR: Parsing error
 url : https://ect-truck.ru
details : DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:34:07 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ect-telecoms.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ect-telecoms.ru'))])
2023-08-15 23:34:07 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.ect.ru/robots.txt> from <GET https://ect.ru/robots.txt>
2023-08-15 23:34:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-telecoms.ru> (failed 3 times): 500 Internal Server Error
2023-08-15 23:34:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-telecoms.ru> (failed 3 times): 500 Internal Server Error
2023-08-15 23:34:07 [scrapy.core.engine] DEBUG: Crawled (500) <GET https://ect-telecoms.ru> (referer: None)
2023-08-15 23:34:07 [root] ERROR: Parsing error
 url : https://ect-telecoms.ru
details : Ignoring non-200 response
2023-08-15 23:34:07 [root] ERROR: Parsing error
 url : https://ect-telecoms.ru
details : Ignoring non-200 response
2023-08-15 23:34:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.ect.ru/robots.txt> (referer: None)
2023-08-15 23:34:07 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.ect.ru/> from <GET https://ect.ru>
2023-08-15 23:34:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.ect.ru/robots.txt> (referer: None)
2023-08-15 23:34:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ect-travel.ru/> (referer: None)
2023-08-15 23:34:07 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ect-travel.ru/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 7162: invalid continuation byte
2023-08-15 23:34:07 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ect-travel.ru/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 52, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 7162: invalid continuation byte
2023-08-15 23:34:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.ect.ru/> (referer: None)
2023-08-15 23:34:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitceva.ru/robots.txt> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect4e.ru/robots.txt> (referer: None)
2023-08-15 23:34:08 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://ect77.ru/robots.txt> (referer: None)
2023-08-15 23:34:08 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:34:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsy.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect77.ru> (referer: None)
2023-08-15 23:34:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsystems.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect4e.ru> (referer: None)
2023-08-15 23:34:10 [scrapy.extensions.logstats] INFO: Crawled 211 pages (at 211 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:34:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsy.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecta.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect4e.ru/contacts/> (referer: https://ect4e.ru)
2023-08-15 23:34:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsystems.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsystems.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsystems.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsystems.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ectaco.ru/robots.txt> (referer: None)
2023-08-15 23:34:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ectaco.ru> (referer: None)
2023-08-15 23:34:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:11 [root] ERROR: Parsing error
 url : https://ecsv.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:11 [root] ERROR: Parsing error
 url : https://ecsv.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecta.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsy.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsy.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ectaco.ru/support/> (referer: https://ectaco.ru)
2023-08-15 23:34:13 [root] ERROR: Parsing error
 url : https://ecsy.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:13 [root] ERROR: Parsing error
 url : https://ecsy.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsystems.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecta.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecta.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecta.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecta.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsystems.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecta.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsystems.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsystems.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:18 [root] ERROR: Parsing error
 url : https://ecsystems.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:18 [root] ERROR: Parsing error
 url : https://ecsystems.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecta.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecta.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecta.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:22 [root] ERROR: Parsing error
 url : https://ecta.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:22 [root] ERROR: Parsing error
 url : https://ecta.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:34:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroland.ru/robots.txt> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zait.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zait.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zait.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zait.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect57.ru/robots.txt> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitceva.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitceva.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitceva.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitceva.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ectastart.ru/robots.txt> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroland.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroland.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroland.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroland.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zait.ru> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect57.ru/robots.txt> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitceva.ru> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:34:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ectastart.ru/robots.txt> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroland.ru> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zait.ru> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:10 [scrapy.extensions.logstats] INFO: Crawled 215 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:35:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect57.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect57.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ect57.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ect57.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitceva.ru> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ectastart.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ectastart.ru/robots.txt> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ectastart.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ectastart.ru/robots.txt>: TCP connection timed out: 10060:     , ..          ,       -     ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroland.ru> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zait.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zait.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:31 [root] ERROR: Parsing error
 url : https://zait.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:31 [root] ERROR: Parsing error
 url : https://zait.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect57.ru> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitceva.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitceva.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:32 [root] ERROR: Parsing error
 url : https://zaitceva.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:32 [root] ERROR: Parsing error
 url : https://zaitceva.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ectastart.ru> (failed 1 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroland.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroland.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:47 [root] ERROR: Parsing error
 url : https://zveroland.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:47 [root] ERROR: Parsing error
 url : https://zveroland.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect57.ru> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:35:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ectastart.ru> (failed 2 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:36:10 [scrapy.extensions.logstats] INFO: Crawled 215 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:36:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenk.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 180.0 seconds..
2023-08-15 23:36:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect57.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:36:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect57.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:36:13 [root] ERROR: Parsing error
 url : https://ect57.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:36:13 [root] ERROR: Parsing error
 url : https://ect57.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:36:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ectastart.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:36:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ectastart.ru> (failed 3 times): TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:36:17 [root] ERROR: Parsing error
 url : https://ectastart.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:36:17 [root] ERROR: Parsing error
 url : https://ectastart.ru
details : TCP connection timed out: 10060:     , ..          ,       -     ..
2023-08-15 23:37:10 [scrapy.extensions.logstats] INFO: Crawled 215 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:38:10 [scrapy.extensions.logstats] INFO: Crawled 215 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:39:10 [scrapy.extensions.logstats] INFO: Crawled 215 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:39:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenk.ru/robots.txt> (failed 2 times): User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 180.0 seconds..
2023-08-15 23:39:30 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2023-08-15 23:39:30 [scrapy.core.engine] INFO: Closing spider (shutdown)
2023-08-15 23:39:35 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2023-08-15 23:39:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenk.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:39:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenk.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:39:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenk.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:39:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenk.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:42:21 [scrapy.utils.log] INFO: Scrapy 2.10.0 started (bot: maininfo)
2023-08-15 23:42:21 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Windows-10-10.0.19043-SP0
2023-08-15 23:42:21 [scrapy.addons] INFO: Enabled addons:
[]
2023-08-15 23:42:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'maininfo',
 'DOWNLOAD_DELAY': 3,
 'DOWNLOAD_TIMEOUT': 10,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'Results/scrapy_log.txt',
 'LOG_LEVEL': 'ERROR',
 'NEWSPIDER_MODULE': 'maininfo.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['maininfo.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
2023-08-15 23:42:21 [asyncio] DEBUG: Using selector: SelectSelector
2023-08-15 23:42:21 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-08-15 23:42:21 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2023-08-15 23:42:21 [scrapy.extensions.telnet] INFO: Telnet Password: 3b745b6310a5f528
2023-08-15 23:42:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-08-15 23:42:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-08-15 23:42:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-08-15 23:42:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-08-15 23:42:21 [scrapy.core.engine] INFO: Spider opened
2023-08-15 23:42:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:42:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-08-15 23:42:21 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-cruise.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-cruise.ru'))])
2023-08-15 23:42:21 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-cruises.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-cruises.ru'))])
2023-08-15 23:42:21 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-desk.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-desk.ru'))])
2023-08-15 23:42:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://booking-cruise.ru/robots.txt> (referer: None)
2023-08-15 23:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booker-spb.ru/robots.txt> (referer: None)
2023-08-15 23:42:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://booking-desk.ru/robots.txt> (referer: None)
2023-08-15 23:42:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://booking-cruises.ru/robots.txt> (referer: None)
2023-08-15 23:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artisticswimming.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dogs.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dog.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistidian.ru/robots.txt> (referer: None)
2023-08-15 23:42:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booking-dubrovka.ru/robots.txt> (referer: None)
2023-08-15 23:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistik.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-crm.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-desk.ru> (failed 1 times): 500 Internal Server Error
2023-08-15 23:42:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistidian.ru> (referer: None)
2023-08-15 23:42:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booking-dubrovka.ru> (referer: None)
2023-08-15 23:42:24 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "artistik55.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'artistik55.ru'))])
2023-08-15 23:42:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistik55.ru/robots.txt> (referer: None)
2023-08-15 23:42:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://artistik55.ru>
2023-08-15 23:42:24 [root] ERROR: Parsing error
 url : https://artistik55.ru
details : Forbidden by robots.txt
2023-08-15 23:42:24 [root] ERROR: Parsing error
 url : https://artistik55.ru
details : Forbidden by robots.txt
2023-08-15 23:42:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistika-gym.ru/robots.txt> (referer: None)
2023-08-15 23:42:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artisticswimming.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotel-dubrovka.ru/robots.txt> (referer: None)
2023-08-15 23:42:25 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-cruises.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-cruises.ru'))])
2023-08-15 23:42:25 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://booking-cruises.ru> (referer: None)
2023-08-15 23:42:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistina.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:25 [root] ERROR: Parsing error
 url : https://booking-cruises.ru
details : Ignoring non-200 response
2023-08-15 23:42:25 [root] ERROR: Parsing error
 url : https://booking-cruises.ru
details : Ignoring non-200 response
2023-08-15 23:42:25 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "booking-cruise.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'booking-cruise.ru'))])
2023-08-15 23:42:25 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://booking-cruise.ru> (referer: None)
2023-08-15 23:42:25 [root] ERROR: Parsing error
 url : https://booking-cruise.ru
details : Ignoring non-200 response
2023-08-15 23:42:25 [root] ERROR: Parsing error
 url : https://booking-cruise.ru
details : Ignoring non-200 response
2023-08-15 23:42:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dog.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dogs.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booker-spb.ru> (referer: None)
2023-08-15 23:42:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistineer.ru/robots.txt> (referer: None)
2023-08-15 23:42:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistik.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-crm.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artisticswimming.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artisticswimming.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artisticswimming.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artisticswimming.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistika-gym.ru> (referer: None)
2023-08-15 23:42:28 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://artistika-gym.ru#contacts> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-08-15 23:42:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-desk.ru> (failed 2 times): 500 Internal Server Error
2023-08-15 23:42:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistina.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotel-dubrovka.ru/kontakty.html> (referer: https://booking-dubrovka.ru)
2023-08-15 23:42:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistik.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistik.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:28 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistik.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:28 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistik.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dogs.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dogs.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-dogs.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-dogs.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://booker-spb.ru/contacts> from <GET https://booker-spb.ru/contacts.html>
2023-08-15 23:42:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistino.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dog.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dog.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-dog.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: booking-dog.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-dog.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: booking-dog.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistium.ru/robots.txt> (referer: None)
2023-08-15 23:42:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistineer.ru> (referer: None)
2023-08-15 23:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistivanov.ru/robots.txt> (referer: None)
2023-08-15 23:42:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-crm.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-crm.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-crm.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://booking-crm.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistique.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:31 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "artistizm.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'artistizm.ru'))])
2023-08-15 23:42:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artisticswimming.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistizm.ru/robots.txt> (referer: None)
2023-08-15 23:42:31 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://artistizm.ru>
2023-08-15 23:42:31 [root] ERROR: Parsing error
 url : https://artistizm.ru
details : Forbidden by robots.txt
2023-08-15 23:42:31 [root] ERROR: Parsing error
 url : https://artistizm.ru
details : Forbidden by robots.txt
2023-08-15 23:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglish.ru/robots.txt> (referer: None)
2023-08-15 23:42:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dogs.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-desk.ru> (failed 3 times): 500 Internal Server Error
2023-08-15 23:42:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-desk.ru> (failed 3 times): 500 Internal Server Error
2023-08-15 23:42:32 [scrapy.core.engine] DEBUG: Crawled (500) <GET https://booking-desk.ru> (referer: None)
2023-08-15 23:42:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://booker-spb.ru/contacts> (referer: https://booker-spb.ru)
2023-08-15 23:42:32 [root] ERROR: Parsing error
 url : https://booking-desk.ru
details : Ignoring non-200 response
2023-08-15 23:42:32 [root] ERROR: Parsing error
 url : https://booking-desk.ru
details : Ignoring non-200 response
2023-08-15 23:42:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglishacademy.ru/robots.txt> (referer: None)
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 207 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 212 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 258 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 266 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 267 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 269 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 275 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 276 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 287 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 288 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 290 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 300 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 305 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 307 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 308 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 314 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 319 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 323 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 325 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 326 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 331 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 334 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 335 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 343 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 345 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 349 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 350 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 351 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 352 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 353 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 354 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 361 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 362 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 365 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 367 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 371 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 375 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 379 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 380 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 389 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 405 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 406 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 415 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 432 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 887 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 888 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 889 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 890 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 891 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 892 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 893 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 899 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 900 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 904 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 905 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 906 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 907 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 908 without any user agent to enforce it on.
2023-08-15 23:42:32 [protego] DEBUG: Rule at line 909 without any user agent to enforce it on.
2023-08-15 23:42:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglishvibes.ru/robots.txt> (referer: None)
2023-08-15 23:42:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistina.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistina.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistina.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: artistina.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistina.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: artistina.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistik.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.freni.ru/robots.txt> from <GET https://freni.ru/robots.txt>
2023-08-15 23:42:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dog.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistino.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistivanov.ru> (referer: None)
2023-08-15 23:42:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-crm.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistium.ru> (referer: None)
2023-08-15 23:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglish.ru> (referer: None)
2023-08-15 23:42:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://frenglish.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 241: invalid start byte
2023-08-15 23:42:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://frenglish.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 241: invalid start byte
2023-08-15 23:42:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistique.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://freniarus.ru/robots.txt> (referer: None)
2023-08-15 23:42:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistina.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artisticswimming.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglishvibes.ru> (referer: None)
2023-08-15 23:42:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistik.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-ace.ru/robots.txt> (referer: None)
2023-08-15 23:42:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dogs.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenglishacademy.ru> (referer: None)
2023-08-15 23:42:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-aqua.ru/robots.txt> (referer: None)
2023-08-15 23:42:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.freni.ru/robots.txt> (referer: None)
2023-08-15 23:42:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-hvac.ru/robots.txt> (referer: None)
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-dog.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistino.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistino.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistino.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistino.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-lift.ru/robots.txt> (referer: None)
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistique.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistique.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistique.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://artistique.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://freniarus.ru> (referer: None)
2023-08-15 23:42:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-ace.ru> (referer: None)
2023-08-15 23:42:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://artistivanov.ru/contacts> (referer: https://artistivanov.ru)
2023-08-15 23:42:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dogs.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dogs.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mega.ru/robots.txt> (referer: None)
2023-08-15 23:42:39 [root] ERROR: Parsing error
 url : https://booking-dogs.ru
details : DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:39 [root] ERROR: Parsing error
 url : https://booking-dogs.ru
details : DNS lookup failed: no results for hostname lookup: booking-dogs.ru.
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artisticswimming.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artisticswimming.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://booking-crm.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic.ru/robots.txt> (referer: None)
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistina.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:40 [root] ERROR: Parsing error
 url : https://artisticswimming.ru
details : DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:40 [root] ERROR: Parsing error
 url : https://artisticswimming.ru
details : DNS lookup failed: no results for hostname lookup: artisticswimming.ru.
2023-08-15 23:42:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mini.ru/robots.txt> (referer: None)
2023-08-15 23:42:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-aqua.ru> (referer: None)
2023-08-15 23:42:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-hvac.ru> (referer: None)
2023-08-15 23:42:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://frenkbit.ru/robots.txt> (referer: None)
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-08-15 23:42:40 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic-aqua.ru/kontakty-frenic-aqua.html>
2023-08-15 23:42:40 [root] ERROR: Parsing error
 url : https://frenic-aqua.ru/kontakty-frenic-aqua.html
details : Forbidden by robots.txt
2023-08-15 23:42:40 [root] ERROR: Parsing error
 url : https://frenic-aqua.ru/kontakty-frenic-aqua.html
details : Forbidden by robots.txt
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.freni.ru/> from <GET https://freni.ru>
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistik.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistik.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.freni.ru/robots.txt> (referer: None)
2023-08-15 23:42:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkel.ru/robots.txt> (referer: None)
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic-hvac.ru/kontakty-frenic-hvac.html>
2023-08-15 23:42:40 [root] ERROR: Parsing error
 url : https://artistik.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:40 [root] ERROR: Parsing error
 url : https://artistik.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkelcars.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:42:40 [root] ERROR: Parsing error
 url : https://frenic-hvac.ru/kontakty-frenic-hvac.html
details : Forbidden by robots.txt
2023-08-15 23:42:40 [root] ERROR: Parsing error
 url : https://frenic-hvac.ru/kontakty-frenic-hvac.html
details : Forbidden by robots.txt
2023-08-15 23:42:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistino.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dog.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-dog.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:42 [root] ERROR: Parsing error
 url : https://booking-dog.ru
details : DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:42 [root] ERROR: Parsing error
 url : https://booking-dog.ru
details : DNS lookup failed: no results for hostname lookup: booking-dog.ru.
2023-08-15 23:42:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mega.ru> (referer: None)
2023-08-15 23:42:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-lift.ru> (referer: None)
2023-08-15 23:42:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mini.ru> (referer: None)
2023-08-15 23:42:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-ace.ru/kontakty-frenic-ace.html> (referer: https://frenic-ace.ru)
2023-08-15 23:42:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkelceramics.ru/robots.txt> (referer: None)
2023-08-15 23:42:42 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic-lift.ru/kontakt.html>
2023-08-15 23:42:42 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic-mega.ru/kontakty-frenic-mega.html>
2023-08-15 23:42:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenki.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:42 [root] ERROR: Parsing error
 url : https://frenic-lift.ru/kontakt.html
details : Forbidden by robots.txt
2023-08-15 23:42:42 [root] ERROR: Parsing error
 url : https://frenic-lift.ru/kontakt.html
details : Forbidden by robots.txt
2023-08-15 23:42:42 [root] ERROR: Parsing error
 url : https://frenic-mega.ru/kontakty-frenic-mega.html
details : Forbidden by robots.txt
2023-08-15 23:42:42 [root] ERROR: Parsing error
 url : https://frenic-mega.ru/kontakty-frenic-mega.html
details : Forbidden by robots.txt
2023-08-15 23:42:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistique.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-crm.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://booking-crm.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:43 [root] ERROR: Parsing error
 url : https://booking-crm.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:43 [root] ERROR: Parsing error
 url : https://booking-crm.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic.ru> (referer: None)
2023-08-15 23:42:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.freni.ru/> (referer: None)
2023-08-15 23:42:44 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://frenic.ru/kontakty.html>
2023-08-15 23:42:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkiee.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:42:44 [root] ERROR: Parsing error
 url : https://frenic.ru/kontakty.html
details : Forbidden by robots.txt
2023-08-15 23:42:44 [root] ERROR: Parsing error
 url : https://frenic.ru/kontakty.html
details : Forbidden by robots.txt
2023-08-15 23:42:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.freni.ru/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf2 in position 22: invalid continuation byte
2023-08-15 23:42:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.freni.ru/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf2 in position 22: invalid continuation byte
2023-08-15 23:42:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistina.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistina.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:44 [root] ERROR: Parsing error
 url : https://artistina.ru
details : DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:44 [root] ERROR: Parsing error
 url : https://artistina.ru
details : DNS lookup failed: no results for hostname lookup: artistina.ru.
2023-08-15 23:42:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkele.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkel.ru> (referer: None)
2023-08-15 23:42:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkbit.ru> (referer: None)
2023-08-15 23:42:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit-pro.ru/robots.txt> (referer: None)
2023-08-15 23:42:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistino.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkelcars.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:42:45 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "frenkit.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'frenkit.ru'))])
2023-08-15 23:42:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit.ru/robots.txt> (referer: None)
2023-08-15 23:42:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkelceramics.ru> (referer: None)
2023-08-15 23:42:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenic-mini.ru/kontakty-fuji-electric-frenic-mini.html> (referer: https://frenic-mini.ru)
2023-08-15 23:42:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenklakh.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:42:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenki.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkiee.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:42:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://artistique.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkele.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit-pro.ru> (referer: None)
2023-08-15 23:42:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvrn.ru/robots.txt> (referer: None)
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkelcars.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkelcars.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkelcars.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkelcars.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:42:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit.ru> (referer: None)
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistino.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistino.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvshira.ru/robots.txt> (referer: None)
2023-08-15 23:42:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://frenkit.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 697: invalid continuation byte
2023-08-15 23:42:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://frenkit.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 697: invalid continuation byte
2023-08-15 23:42:49 [root] ERROR: Parsing error
 url : https://artistino.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:49 [root] ERROR: Parsing error
 url : https://artistino.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenki.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenki.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenki.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenki.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenklakh.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:42:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenk.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:42:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistique.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://artistique.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:50 [root] ERROR: Parsing error
 url : https://artistique.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:50 [root] ERROR: Parsing error
 url : https://artistique.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremwood.ru/robots.txt> (referer: None)
2023-08-15 23:42:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkiee.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:42:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkiee.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:42:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkiee.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: frenkiee.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:42:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkiee.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: frenkiee.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:42:51 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teremyg.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teremyg.ru'))])
2023-08-15 23:42:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://teremyg.ru/> from <GET https://teremyg.ru/robots.txt>
2023-08-15 23:42:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkele.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkele.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkele.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenkele.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://frenkit-pro.ru/contacts> (referer: https://frenkit-pro.ru)
2023-08-15 23:42:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvrn.ru> (referer: None)
2023-08-15 23:42:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenklakh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:42:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenklakh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:42:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenklakh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: frenklakh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:42:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenklakh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: frenklakh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:42:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkelcars.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:42:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremzdravia.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:42:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvshira.ru> (referer: None)
2023-08-15 23:42:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremzaim.ru/robots.txt> (referer: None)
2023-08-15 23:42:53 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://teremzaim.ru>
2023-08-15 23:42:53 [root] ERROR: Parsing error
 url : https://teremzaim.ru
details : Forbidden by robots.txt
2023-08-15 23:42:53 [root] ERROR: Parsing error
 url : https://teremzaim.ru
details : Forbidden by robots.txt
2023-08-15 23:42:53 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:42:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teren-control.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:42:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenki.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremwood.ru> (referer: None)
2023-08-15 23:42:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremyg.ru/> (referer: None)
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 53 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 111 without any user agent to enforce it on.
2023-08-15 23:42:55 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-08-15 23:42:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkiee.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:42:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremvrn.ru/index.php?route=information/contact> (referer: https://teremvrn.ru)
2023-08-15 23:42:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teren.ru/robots.txt> (referer: None)
2023-08-15 23:42:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenklakh.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:42:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkele.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:42:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkelcars.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:42:56 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:42:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teren-control.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:42:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenki.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremzdravia.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:42:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremwood.ru/contact> (referer: https://teremwood.ru)
2023-08-15 23:42:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkiee.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:42:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terena.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:42:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teren.ru> (referer: None)
2023-08-15 23:42:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenki.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenki.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:59 [root] ERROR: Parsing error
 url : https://frenki.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:59 [root] ERROR: Parsing error
 url : https://frenki.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:42:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenberg.ru/robots.txt> (referer: None)
2023-08-15 23:42:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremvvk.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://teremvvk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:42:59 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teren-control.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teren-control.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teren-control.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teren-control.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenk.ru/robots.txt> (failed 2 times): User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkelcars.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkelcars.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenklakh.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremzdravia.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremzdravia.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teremzdravia.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teremzdravia.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terencegrech.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:00 [root] ERROR: Parsing error
 url : https://frenkelcars.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:43:00 [root] ERROR: Parsing error
 url : https://frenkelcars.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'sslv3 alert handshake failure')]>]
2023-08-15 23:43:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenkele.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teremyg.ru> (referer: None)
2023-08-15 23:43:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkiee.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:43:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkiee.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:43:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terence.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:02 [root] ERROR: Parsing error
 url : https://frenkiee.ru
details : DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:43:02 [root] ERROR: Parsing error
 url : https://frenkiee.ru
details : DNS lookup failed: no results for hostname lookup: frenkiee.ru.
2023-08-15 23:43:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenceuniform.ru/robots.txt> (referer: None)
2023-08-15 23:43:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terena.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenberg.ru> (referer: None)
2023-08-15 23:43:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenchin.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terencegrech.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremzdravia.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:04 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:43:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenklakh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:43:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenklakh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:43:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teren-control.ru> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:04 [root] ERROR: Parsing error
 url : https://frenklakh.ru
details : DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:43:04 [root] ERROR: Parsing error
 url : https://frenklakh.ru
details : DNS lookup failed: no results for hostname lookup: frenklakh.ru.
2023-08-15 23:43:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkele.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenkele.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:04 [root] ERROR: Parsing error
 url : https://frenkele.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:04 [root] ERROR: Parsing error
 url : https://frenkele.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terena.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terena.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:05 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terena.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terena.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:05 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terena.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terena.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terence.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenceuniform.ru> (referer: None)
2023-08-15 23:43:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenda.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:07 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:43:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teren-control.ru> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenchin.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremzdravia.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terencegrech.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terencegrech.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:07 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terencegrech.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terencegrech.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:07 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terencegrech.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terencegrech.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terena.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terendina.ru/robots.txt> (referer: None)
2023-08-15 23:43:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenergoprom.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremvvk.ru/robots.txt> (failed 2 times): User timeout caused connection failure.
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenk.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenk.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenk.ru/robots.txt>: User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://frenk.ru/robots.txt>: User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://frenk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terence.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terence.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terence.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terence.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenda.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terencegrech.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:10 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "teren-control.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'teren-control.ru'))])
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teren-control.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teren-control.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:11 [root] ERROR: Parsing error
 url : https://teren-control.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:11 [root] ERROR: Parsing error
 url : https://teren-control.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:43:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terendina.ru> (referer: None)
2023-08-15 23:43:11 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:43:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenga.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenchin.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenchin.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenchin.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenchin.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenchin.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenchin.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremzdravia.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremzdravia.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengh.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:12 [root] ERROR: Parsing error
 url : https://teremzdravia.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:12 [root] ERROR: Parsing error
 url : https://teremzdravia.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereng.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terena.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenergoprom.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenda.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenda.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenda.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenda.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terence.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terencegrech.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenchin.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:15 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:43:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenga.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenergoprom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenergoprom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenergoprom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenergoprom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenghi.ru/robots.txt> (referer: None)
2023-08-15 23:43:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengh.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereng.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terena.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terena.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:17 [root] ERROR: Parsing error
 url : https://terena.ru
details : DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:17 [root] ERROR: Parsing error
 url : https://terena.ru
details : DNS lookup failed: no results for hostname lookup: terena.ru.
2023-08-15 23:43:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terencegrech.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terencegrech.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenda.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:17 [root] ERROR: Parsing error
 url : https://terencegrech.ru
details : DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:17 [root] ERROR: Parsing error
 url : https://terencegrech.ru
details : DNS lookup failed: no results for hostname lookup: terencegrech.ru.
2023-08-15 23:43:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terence.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://terenghi.ru> (referer: None)
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereni4ev.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenergoprom.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremvvk.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://teremvvk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremvvk.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://teremvvk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teremvvk.ru/robots.txt>: User timeout caused connection failure: Getting https://teremvvk.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://teremvvk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://teremvvk.ru/robots.txt>: User timeout caused connection failure: Getting https://teremvvk.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://teremvvk.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:43:19 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenchin.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenga.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenga.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenga.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenga.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengi.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenik.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenk.ru> (failed 1 times): User timeout caused connection failure: Getting https://frenk.ru took longer than 10.0 seconds..
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terengh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terengh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terengh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terengh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereng.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereng.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://tereng.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://tereng.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-group.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenda.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:21 [scrapy.extensions.logstats] INFO: Crawled 85 pages (at 85 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:43:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terence.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terence.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:22 [root] ERROR: Parsing error
 url : https://terence.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:22 [root] ERROR: Parsing error
 url : https://terence.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenchin.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenchin.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-sa.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:22 [root] ERROR: Parsing error
 url : https://terenchin.ru
details : DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:22 [root] ERROR: Parsing error
 url : https://terenchin.ru
details : DNS lookup failed: no results for hostname lookup: terenchin.ru.
2023-08-15 23:43:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengi.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:23 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:43:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenga.ru> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereni4ev.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenergoprom.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenik.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-group.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenda.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenda.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:24 [root] ERROR: Parsing error
 url : https://terenda.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:24 [root] ERROR: Parsing error
 url : https://terenda.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengh.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereng.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:25 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:43:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenga.ru> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengi.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengi.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terengi.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terengi.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-sa.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-group.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-group.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov-group.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov-group.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov-group.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov-group.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereni4ev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereni4ev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://tereni4ev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://tereni4ev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov2111.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenergoprom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenergoprom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:27 [root] ERROR: Parsing error
 url : https://terenergoprom.ru
details : DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:27 [root] ERROR: Parsing error
 url : https://terenergoprom.ru
details : DNS lookup failed: no results for hostname lookup: terenergoprom.ru.
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengi.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengh.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenik.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenik.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:28 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenik.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenik.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:28 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://terenik.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: terenik.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurova-beauty.ru/robots.txt> (referer: None)
2023-08-15 23:43:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereng.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremvvk.ru> (failed 1 times): User timeout caused connection failure.
2023-08-15 23:43:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:29 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "terenga.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'terenga.ru'))])
2023-08-15 23:43:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenga.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenga.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-sa.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-sa.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov-sa.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov-sa.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurova.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:30 [root] ERROR: Parsing error
 url : https://terenga.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:30 [root] ERROR: Parsing error
 url : https://terenga.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2023-08-15 23:43:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://frenk.ru> (failed 2 times): User timeout caused connection failure: Getting https://frenk.ru took longer than 10.0 seconds..
2023-08-15 23:43:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenik.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereni4ev.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov2111.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereng.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereng.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:31 [root] ERROR: Parsing error
 url : https://tereng.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:31 [root] ERROR: Parsing error
 url : https://tereng.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-group.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurova-beauty.ru> (referer: None)
2023-08-15 23:43:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaa.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terengi.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:32 [root] ERROR: Parsing error
 url : https://terengh.ru
details : DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:32 [root] ERROR: Parsing error
 url : https://terengh.ru
details : DNS lookup failed: no results for hostname lookup: terengh.ru.
2023-08-15 23:43:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurova.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://terenik.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://tereni4ev.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-sa.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaceramics.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-group.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengi.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terengi.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovamaks.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:35 [root] ERROR: Parsing error
 url : https://terengi.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:35 [root] ERROR: Parsing error
 url : https://terengi.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov2111.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov2111.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov2111.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov2111.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurov2111.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurov2111.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaa.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovgroup.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereni4ev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://tereni4ev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:37 [root] ERROR: Parsing error
 url : https://tereni4ev.ru
details : DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:37 [root] ERROR: Parsing error
 url : https://tereni4ev.ru
details : DNS lookup failed: no results for hostname lookup: tereni4ev.ru.
2023-08-15 23:43:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenik.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://terenik.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovich.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:37 [root] ERROR: Parsing error
 url : https://terenik.ru
details : DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:37 [root] ERROR: Parsing error
 url : https://terenik.ru
details : DNS lookup failed: no results for hostname lookup: terenik.ru.
2023-08-15 23:43:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaceramics.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov2111.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurova.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov-sa.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovamaks.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-group.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-group.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://teremvvk.ru> (failed 2 times): User timeout caused connection failure.
2023-08-15 23:43:39 [root] ERROR: Parsing error
 url : https://yurov-group.ru
details : DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:39 [root] ERROR: Parsing error
 url : https://yurov-group.ru
details : DNS lookup failed: no results for hostname lookup: yurov-group.ru.
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenk.ru> (failed 3 times): User timeout caused connection failure: Getting https://frenk.ru took longer than 10.0 seconds..
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://frenk.ru> (failed 3 times): User timeout caused connection failure: Getting https://frenk.ru took longer than 10.0 seconds..
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaa.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaa.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovaa.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovaa.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovaa.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovaa.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:40 [root] ERROR: Parsing error
 url : https://frenk.ru
details : User timeout caused connection failure: Getting https://frenk.ru took longer than 10.0 seconds..
2023-08-15 23:43:40 [root] ERROR: Parsing error
 url : https://frenk.ru
details : User timeout caused connection failure: Getting https://frenk.ru took longer than 10.0 seconds..
2023-08-15 23:43:40 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://yurovkurs.ru/robots.txt> (referer: None)
2023-08-15 23:43:40 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:43:40 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-08-15 23:43:40 [protego] DEBUG: Rule at line 5 without any user agent to enforce it on.
2023-08-15 23:43:40 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnik.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaceramics.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaceramics.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovaceramics.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:40 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovaceramics.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovgroup.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovich.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-sa.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov-sa.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnikova.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:41 [root] ERROR: Parsing error
 url : https://yurov-sa.ru
details : DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:41 [root] ERROR: Parsing error
 url : https://yurov-sa.ru
details : DNS lookup failed: no results for hostname lookup: yurov-sa.ru.
2023-08-15 23:43:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:42 [root] ERROR: Parsing error
 url : https://yurov.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:42 [root] ERROR: Parsing error
 url : https://yurov.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 unrecognized name')]>]
2023-08-15 23:43:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurova.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurov2111.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovamaks.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovamaks.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovamaks.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovamaks.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:43 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "yurovo.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'yurovo.ru'))])
2023-08-15 23:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://yurovo.ru/robots.txt> (referer: None)
2023-08-15 23:43:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovgroup.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovgroup.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovgroup.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovgroup.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovich.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovich.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovich.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovich.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovich.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovich.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaa.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://yurovskaya.ru/robots.txt> (referer: None)
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 97 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 105 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 108 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 118 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 132 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 145 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 146 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 151 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 181 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 182 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 189 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 192 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 193 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 200 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 201 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 204 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 205 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 208 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 210 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 215 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 266 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 281 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 307 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 316 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 329 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 331 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 334 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 335 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 347 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 388 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 389 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 397 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 402 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 403 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 404 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 405 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 406 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-08-15 23:43:44 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskih.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaceramics.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnik.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:43:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnikova.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:44 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://yurovkurs.ru> (referer: None)
2023-08-15 23:43:45 [root] ERROR: Parsing error
 url : https://yurovkurs.ru
details : Ignoring non-200 response
2023-08-15 23:43:45 [root] ERROR: Parsing error
 url : https://yurovkurs.ru
details : Ignoring non-200 response
2023-08-15 23:43:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskikh.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:43:45 [root] ERROR: Parsing error
 url : https://yurova.ru
details : DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:45 [root] ERROR: Parsing error
 url : https://yurova.ru
details : DNS lookup failed: no results for hostname lookup: yurova.ru.
2023-08-15 23:43:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov2111.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurov2111.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:46 [root] ERROR: Parsing error
 url : https://yurov2111.ru
details : DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:46 [root] ERROR: Parsing error
 url : https://yurov2111.ru
details : DNS lookup failed: no results for hostname lookup: yurov2111.ru.
2023-08-15 23:43:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovich.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovgroup.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovo.ru> (referer: None)
2023-08-15 23:43:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaa.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovamaks.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovskiy-kirill.ru/robots.txt> (referer: None)
2023-08-15 23:43:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovskaya.ru> (referer: None)
2023-08-15 23:43:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovaceramics.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskiy.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:43:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskih.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnik.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnik.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovnik.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovnik.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovnik.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovnik.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnikova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnikova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovnikova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovnikova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovsky.ru/robots.txt> (referer: None)
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskikh.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremvvk.ru> (failed 3 times): User timeout caused connection failure.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://teremvvk.ru> (failed 3 times): User timeout caused connection failure.
2023-08-15 23:43:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovstudio.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:43:49 [root] ERROR: Parsing error
 url : https://teremvvk.ru
details : User timeout caused connection failure.
2023-08-15 23:43:49 [root] ERROR: Parsing error
 url : https://teremvvk.ru
details : User timeout caused connection failure.
2023-08-15 23:43:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovich.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaa.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaa.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:50 [root] ERROR: Parsing error
 url : https://yurovaa.ru
details : DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:50 [root] ERROR: Parsing error
 url : https://yurovaa.ru
details : DNS lookup failed: no results for hostname lookup: yurovaa.ru.
2023-08-15 23:43:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovgroup.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskih.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:43:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskih.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:43:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskih.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskih.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:43:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskih.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskih.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:43:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovwed.ru/robots.txt> (referer: None)
2023-08-15 23:43:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovskiy-kirill.ru> (referer: None)
2023-08-15 23:43:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaceramics.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovaceramics.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovamaks.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskiy.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:43:52 [root] ERROR: Parsing error
 url : https://yurovaceramics.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:52 [root] ERROR: Parsing error
 url : https://yurovaceramics.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnik.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:43:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnikova.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskikh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:43:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskikh.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:43:53 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskikh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:43:53 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskikh.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:43:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaisy.ru/robots.txt> (referer: None)
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovstudio.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurp.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovgroup.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovgroup.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskiy.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskiy.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskiy.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovskiy.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovich.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovich.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:54 [root] ERROR: Parsing error
 url : https://yurovgroup.ru
details : DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:54 [root] ERROR: Parsing error
 url : https://yurovgroup.ru
details : DNS lookup failed: no results for hostname lookup: yurovgroup.ru.
2023-08-15 23:43:54 [root] ERROR: Parsing error
 url : https://yurovich.ru
details : DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:54 [root] ERROR: Parsing error
 url : https://yurovich.ru
details : DNS lookup failed: no results for hostname lookup: yurovich.ru.
2023-08-15 23:43:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zait-service.ru/robots.txt> (referer: None)
2023-08-15 23:43:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovsky.ru> (referer: None)
2023-08-15 23:43:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaita.ru/robots.txt> (referer: None)
2023-08-15 23:43:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovwed.ru> (referer: None)
2023-08-15 23:43:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskih.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:43:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovsky.ru/kontakty/> (referer: https://yurovsky.ru)
2023-08-15 23:43:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnikova.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovnik.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:43:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovamaks.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovamaks.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:56 [root] ERROR: Parsing error
 url : https://yurovamaks.ru
details : DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:56 [root] ERROR: Parsing error
 url : https://yurovamaks.ru
details : DNS lookup failed: no results for hostname lookup: yurovamaks.ru.
2023-08-15 23:43:56 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitbiz55.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitbiz55.ru'))])
2023-08-15 23:43:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://zaitbiz55.ru/robots.txt> (referer: None)
2023-08-15 23:43:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurp.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:43:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskikh.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:43:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskiy.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:43:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovstudio.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:43:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovstudio.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:43:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovstudio.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:43:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurovstudio.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:43:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitbudushcheye.ru/robots.txt> (failed 1 times): 503 Service Unavailable
2023-08-15 23:43:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zait-service.ru> (referer: None)
2023-08-15 23:43:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaisy.ru> (referer: None)
2023-08-15 23:43:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitc.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:43:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnikova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnikova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:58 [root] ERROR: Parsing error
 url : https://yurovnikova.ru
details : DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:58 [root] ERROR: Parsing error
 url : https://yurovnikova.ru
details : DNS lookup failed: no results for hostname lookup: yurovnikova.ru.
2023-08-15 23:43:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaita.ru> (referer: None)
2023-08-15 23:43:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskih.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:43:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitbiz55.ru> (referer: None)
2023-08-15 23:44:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitcev.ru/robots.txt> (referer: None)
2023-08-15 23:44:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yurovwed.ru/contacts> (referer: https://yurovwed.ru)
2023-08-15 23:44:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskiy.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:44:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcev911.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnik.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:44:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovnik.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:44:00 [root] ERROR: Parsing error
 url : https://yurovnik.ru
details : DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:44:00 [root] ERROR: Parsing error
 url : https://yurovnik.ru
details : DNS lookup failed: no results for hostname lookup: yurovnik.ru.
2023-08-15 23:44:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurp.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurp.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurp.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://yurp.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitceva-photo.ru/robots.txt> (referer: None)
2023-08-15 23:44:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovskikh.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:44:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaisy.ru/contacts/> (referer: https://zaisy.ru)
2023-08-15 23:44:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskih.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:44:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskih.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:44:02 [root] ERROR: Parsing error
 url : https://yurovskih.ru
details : DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:44:02 [root] ERROR: Parsing error
 url : https://yurovskih.ru
details : DNS lookup failed: no results for hostname lookup: yurovskih.ru.
2023-08-15 23:44:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovstudio.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:44:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitc.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitbudushcheye.ru/robots.txt> (failed 2 times): 503 Service Unavailable
2023-08-15 23:44:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitcev.ru> (referer: None)
2023-08-15 23:44:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitceva-photo.ru> (referer: None)
2023-08-15 23:44:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskikh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:44:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskikh.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:44:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevpetr.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zait.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://zait.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:04 [root] ERROR: Parsing error
 url : https://yurovskikh.ru
details : DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:44:04 [root] ERROR: Parsing error
 url : https://yurovskikh.ru
details : DNS lookup failed: no results for hostname lookup: yurovskikh.ru.
2023-08-15 23:44:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcev911.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskiy.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:44:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovskiy.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:44:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurp.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:04 [root] ERROR: Parsing error
 url : https://yurovskiy.ru
details : DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:44:04 [root] ERROR: Parsing error
 url : https://yurovskiy.ru
details : DNS lookup failed: no results for hostname lookup: yurovskiy.ru.
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitc.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitc.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitc.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitc.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitc.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitc.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitev.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevmedia.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurovstudio.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitbudushcheye.ru/robots.txt> (failed 3 times): 503 Service Unavailable
2023-08-15 23:44:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitbudushcheye.ru/robots.txt> (failed 3 times): 503 Service Unavailable
2023-08-15 23:44:06 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://zaitbudushcheye.ru/robots.txt> (referer: None)
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-08-15 23:44:06 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-08-15 23:44:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://yurp.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevpetr.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcev911.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcev911.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:08 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcev911.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:08 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcev911.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:09 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitfarm.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitfarm.ru'))])
2023-08-15 23:44:09 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://zaitfarm.ru/robots.txt> (referer: None)
2023-08-15 23:44:09 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:44:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitev.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurp.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurp.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:09 [root] ERROR: Parsing error
 url : https://yurp.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:09 [root] ERROR: Parsing error
 url : https://yurp.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitc.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovstudio.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:44:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://yurovstudio.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:44:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevmedia.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:10 [root] ERROR: Parsing error
 url : https://yurovstudio.ru
details : DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:44:10 [root] ERROR: Parsing error
 url : https://yurovstudio.ru
details : DNS lookup failed: no results for hostname lookup: yurovstudio.ru.
2023-08-15 23:44:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevpetr.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevpetr.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcevpetr.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcevpetr.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitivaiti.ru/robots.txt> (referer: None)
2023-08-15 23:44:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitbudushcheye.ru> (failed 1 times): 503 Service Unavailable
2023-08-15 23:44:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitceva.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://zaitceva.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaiti.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitivit.ru/robots.txt> (referer: None)
2023-08-15 23:44:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcev911.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitfarm.ru> (referer: None)
2023-08-15 23:44:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitochka.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevpetr.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://zaitivaiti.ru/login> from <GET https://zaitivaiti.ru>
2023-08-15 23:44:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitc.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zait.ru/robots.txt> (failed 2 times): User timeout caused connection failure.
2023-08-15 23:44:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevmedia.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevmedia.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcevmedia.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitcevmedia.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitbudushcheye.ru> (failed 2 times): 503 Service Unavailable
2023-08-15 23:44:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitev.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaiti.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitochka.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcev911.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitivit.ru> (referer: None)
2023-08-15 23:44:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitomart.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevpetr.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitc.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitc.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:18 [root] ERROR: Parsing error
 url : https://zaitc.ru
details : DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:18 [root] ERROR: Parsing error
 url : https://zaitc.ru
details : DNS lookup failed: no results for hostname lookup: zaitc.ru.
2023-08-15 23:44:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevmedia.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://zaitivaiti.ru/cms/system/login> from <GET https://zaitivaiti.ru/login>
2023-08-15 23:44:18 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zaitivaiti.ru/cms/system/login>
2023-08-15 23:44:18 [root] ERROR: Parsing error
 url : https://zaitivaiti.ru/cms/system/login
details : Forbidden by robots.txt
2023-08-15 23:44:18 [root] ERROR: Parsing error
 url : https://zaitivaiti.ru/cms/system/login
details : Forbidden by robots.txt
2023-08-15 23:44:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcev911.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcev911.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:19 [root] ERROR: Parsing error
 url : https://zaitcev911.ru
details : DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:19 [root] ERROR: Parsing error
 url : https://zaitcev911.ru
details : DNS lookup failed: no results for hostname lookup: zaitcev911.ru.
2023-08-15 23:44:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevpetr.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevpetr.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitova.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:19 [root] ERROR: Parsing error
 url : https://zaitcevpetr.ru
details : DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:19 [root] ERROR: Parsing error
 url : https://zaitcevpetr.ru
details : DNS lookup failed: no results for hostname lookup: zaitcevpetr.ru.
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitbudushcheye.ru> (failed 3 times): 503 Service Unavailable
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitbudushcheye.ru> (failed 3 times): 503 Service Unavailable
2023-08-15 23:44:20 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://zaitbudushcheye.ru> (referer: None)
2023-08-15 23:44:20 [root] ERROR: Parsing error
 url : https://zaitbudushcheye.ru
details : Ignoring non-200 response
2023-08-15 23:44:20 [root] ERROR: Parsing error
 url : https://zaitbudushcheye.ru
details : Ignoring non-200 response
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitomart.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitochka.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitochka.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitochka.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitochka.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitov.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:20 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitev.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovaelena.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaiti.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaiti.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaiti.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaiti.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:21 [scrapy.extensions.logstats] INFO: Crawled 121 pages (at 36 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:44:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitceva.ru/robots.txt> (failed 2 times): User timeout caused connection failure: Getting https://zaitceva.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitcevmedia.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitova.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitomart.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitomart.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitomart.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitomart.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitomart.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitomart.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitov.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:24 [root] ERROR: Parsing error
 url : https://zaitev.ru
details : DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:24 [root] ERROR: Parsing error
 url : https://zaitev.ru
details : DNS lookup failed: no results for hostname lookup: zaitev.ru.
2023-08-15 23:44:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zait.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://zait.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zait.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://zait.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:24 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zait.ru/robots.txt>: User timeout caused connection failure: Getting https://zait.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://zait.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:24 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zait.ru/robots.txt>: User timeout caused connection failure: Getting https://zait.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://zait.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovrr.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitochka.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:25 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:44:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovaelena.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaiti.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovanazira.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitomart.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevmedia.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitcevmedia.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitov.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitov.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitov.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitov.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:27 [root] ERROR: Parsing error
 url : https://zaitcevmedia.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:27 [root] ERROR: Parsing error
 url : https://zaitcevmedia.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitovstudio.ru/robots.txt> (referer: None)
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitova.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitova.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitova.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitoys.ru/robots.txt> (referer: None)
2023-08-15 23:44:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovanazira.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:28 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:44:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovaelena.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovaelena.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:28 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovaelena.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:28 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovaelena.ru/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaits.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovrr.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitomart.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitochka.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaiti.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitov.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitovstudio.ru> (referer: None)
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovrr.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovrr.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovrr.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovrr.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitoys.ru> (referer: None)
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsburg.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitceva.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://zaitceva.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitceva.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://zaitceva.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitceva.ru/robots.txt>: User timeout caused connection failure: Getting https://zaitceva.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://zaitceva.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitceva.ru/robots.txt>: User timeout caused connection failure: Getting https://zaitceva.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://zaitceva.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitova.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaits.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitochka.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitochka.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:32 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.petrzaytsev.ru> from <GET https://zaitseff.ru/robots.txt>
2023-08-15 23:44:32 [root] ERROR: Parsing error
 url : https://zaitochka.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:32 [root] ERROR: Parsing error
 url : https://zaitochka.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'tlsv1 alert internal error')]>]
2023-08-15 23:44:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovanazira.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovanazira.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovanazira.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitovanazira.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-1.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:32 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovaelena.ru> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitomart.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitomart.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:33 [root] ERROR: Parsing error
 url : https://zaitomart.ru
details : DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:33 [root] ERROR: Parsing error
 url : https://zaitomart.ru
details : DNS lookup failed: no results for hostname lookup: zaitomart.ru.
2023-08-15 23:44:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaiti.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaiti.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:33 [root] ERROR: Parsing error
 url : https://zaiti.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:33 [root] ERROR: Parsing error
 url : https://zaiti.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://zaitsev-consulting.ru/robots.txt> (referer: None)
2023-08-15 23:44:34 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:44:34 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-08-15 23:44:34 [protego] DEBUG: Rule at line 5 without any user agent to enforce it on.
2023-08-15 23:44:34 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zait.ru> (failed 1 times): User timeout caused connection failure: Getting https://zait.ru took longer than 10.0 seconds..
2023-08-15 23:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitov.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovanazira.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitova.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovrr.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsburg.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaits.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaits.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaits.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaits.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaits.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaits.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-done.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.petrzaytsev.ru> (referer: None)
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-08-15 23:44:36 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-08-15 23:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-1.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:37 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovaelena.ru> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-consulting.ru> (referer: None)
2023-08-15 23:44:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-drum.ru/robots.txt> (referer: None)
2023-08-15 23:44:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsburg.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsburg.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsburg.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsburg.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-life.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitov.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitov.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:38 [root] ERROR: Parsing error
 url : https://zaitov.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:38 [root] ERROR: Parsing error
 url : https://zaitov.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitova.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-mail.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:39 [root] ERROR: Parsing error
 url : https://zaitova.ru
details : DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:39 [root] ERROR: Parsing error
 url : https://zaitova.ru
details : DNS lookup failed: no results for hostname lookup: zaitova.ru.
2023-08-15 23:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovanazira.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-done.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitovrr.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaits.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsburg.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitseff.ru> (referer: None)
2023-08-15 23:44:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-1.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-1.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:40 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-1.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:40 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-1.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-nsk.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:41 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zaitovaelena.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zaitovaelena.ru'))])
2023-08-15 23:44:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovaelena.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovaelena.ru> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:41 [root] ERROR: Parsing error
 url : https://zaitovaelena.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:41 [root] ERROR: Parsing error
 url : https://zaitovaelena.ru
details : [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2023-08-15 23:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitceva.ru> (failed 1 times): User timeout caused connection failure: Getting https://zaitceva.ru took longer than 10.0 seconds..
2023-08-15 23:44:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-drum.ru> (referer: None)
2023-08-15 23:44:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-prosluh.ru/robots.txt> (referer: None)
2023-08-15 23:44:42 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zaitsev-prosluh.ru>
2023-08-15 23:44:42 [root] ERROR: Parsing error
 url : https://zaitsev-prosluh.ru
details : Forbidden by robots.txt
2023-08-15 23:44:42 [root] ERROR: Parsing error
 url : https://zaitsev-prosluh.ru
details : Forbidden by robots.txt
2023-08-15 23:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-life.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-mail.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-seo.ru/robots.txt> (referer: None)
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-done.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-done.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-done.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-done.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-top.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsburg.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovanazira.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovanazira.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:43 [root] ERROR: Parsing error
 url : https://zaitovanazira.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:43 [root] ERROR: Parsing error
 url : https://zaitovanazira.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovrr.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitovrr.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:44:43 [root] ERROR: Parsing error
 url : https://zaitovrr.ru
details : DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:43 [root] ERROR: Parsing error
 url : https://zaitovrr.ru
details : DNS lookup failed: no results for hostname lookup: zaitovrr.ru.
2023-08-15 23:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-nsk.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaits.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zait.ru> (failed 2 times): User timeout caused connection failure: Getting https://zait.ru took longer than 10.0 seconds..
2023-08-15 23:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-1.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-top.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-mail.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-mail.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-mail.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-mail.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-seo.ru> (referer: None)
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroclinic.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-life.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-life.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-life.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-life.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsburg.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsburg.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:46 [root] ERROR: Parsing error
 url : https://zaitsburg.ru
details : DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:46 [root] ERROR: Parsing error
 url : https://zaitsburg.ru
details : DNS lookup failed: no results for hostname lookup: zaitsburg.ru.
2023-08-15 23:44:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:44:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-done.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-nsk.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-nsk.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:47 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-nsk.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:47 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-nsk.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroclub.ru/robots.txt> (referer: None)
2023-08-15 23:44:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-top.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-top.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-top.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev-top.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverocomp.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:44:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaits.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaits.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:48 [root] ERROR: Parsing error
 url : https://zaits.ru
details : DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:48 [root] ERROR: Parsing error
 url : https://zaits.ru
details : DNS lookup failed: no results for hostname lookup: zaits.ru.
2023-08-15 23:44:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroclinic.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:44:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-1.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zaitsev-seo.ru/kontakty/> (referer: https://zaitsev-seo.ru)
2023-08-15 23:44:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-mail.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:44:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:44:50 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:44:50 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zaitsev.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zaitsev.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:44:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-life.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-nsk.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverocomp.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:44:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroclinic.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:44:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroclinic.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:44:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroclinic.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:44:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroclinic.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:44:51 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zverodel.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zverodel.ru'))])
2023-08-15 23:44:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://zverodel.ru/robots.txt> from <GET https://zverodel.ru/robots.txt>
2023-08-15 23:44:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitceva.ru> (failed 2 times): User timeout caused connection failure: Getting https://zaitceva.ru took longer than 10.0 seconds..
2023-08-15 23:44:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-done.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodar.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-top.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-1.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-1.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroclub.ru> (referer: None)
2023-08-15 23:44:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodom.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:44:53 [root] ERROR: Parsing error
 url : https://zaitsev-1.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:53 [root] ERROR: Parsing error
 url : https://zaitsev-1.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-1.ru.
2023-08-15 23:44:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-nsk.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-life.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-mail.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:44:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zait.ru> (failed 3 times): User timeout caused connection failure: Getting https://zait.ru took longer than 10.0 seconds..
2023-08-15 23:44:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zait.ru> (failed 3 times): User timeout caused connection failure: Getting https://zait.ru took longer than 10.0 seconds..
2023-08-15 23:44:54 [root] ERROR: Parsing error
 url : https://zait.ru
details : User timeout caused connection failure: Getting https://zait.ru took longer than 10.0 seconds..
2023-08-15 23:44:54 [root] ERROR: Parsing error
 url : https://zait.ru
details : User timeout caused connection failure: Getting https://zait.ru took longer than 10.0 seconds..
2023-08-15 23:44:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroclinic.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:44:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverocomp.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:44:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverocomp.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:44:55 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverocomp.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverocomp.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:44:55 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverocomp.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverocomp.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:44:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodrom.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:44:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-done.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-done.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:55 [root] ERROR: Parsing error
 url : https://zaitsev-done.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:55 [root] ERROR: Parsing error
 url : https://zaitsev-done.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-done.ru.
2023-08-15 23:44:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://zverodel.ru/robots.txt> (referer: None)
2023-08-15 23:44:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroferm.ru/robots.txt> (referer: None)
2023-08-15 23:44:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodar.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:44:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev-top.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-mail.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-mail.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroferma.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:44:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodom.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:44:56 [root] ERROR: Parsing error
 url : https://zaitsev-mail.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:56 [root] ERROR: Parsing error
 url : https://zaitsev-mail.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-mail.ru.
2023-08-15 23:44:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zaitsev.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:44:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-nsk.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-nsk.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:58 [root] ERROR: Parsing error
 url : https://zaitsev-nsk.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:58 [root] ERROR: Parsing error
 url : https://zaitsev-nsk.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-nsk.ru.
2023-08-15 23:44:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-life.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-life.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:58 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zveroff.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zveroff.ru'))])
2023-08-15 23:44:58 [root] ERROR: Parsing error
 url : https://zaitsev-life.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:58 [root] ERROR: Parsing error
 url : https://zaitsev-life.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-life.ru.
2023-08-15 23:44:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroff.ru/robots.txt> (referer: None)
2023-08-15 23:44:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroclinic.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:44:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://zverodel.ru/> from <GET https://zverodel.ru>
2023-08-15 23:44:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverocomp.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:44:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-top.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev-top.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroferm.ru> (referer: None)
2023-08-15 23:44:59 [root] ERROR: Parsing error
 url : https://zaitsev-top.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:59 [root] ERROR: Parsing error
 url : https://zaitsev-top.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev-top.ru.
2023-08-15 23:44:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodrom.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:44:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverogorod.ru/robots.txt> (referer: None)
2023-08-15 23:44:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverograd.ru/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverodom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverodom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodar.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodar.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodar.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodar.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroferma.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:45:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:45:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitsev.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:45:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://zverojaschery.ru/robots.txt> from <GET https://zverojaschery.ru/robots.txt>
2023-08-15 23:45:01 [root] ERROR: Parsing error
 url : https://zaitsev.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:45:01 [root] ERROR: Parsing error
 url : https://zaitsev.ru
details : DNS lookup failed: no results for hostname lookup: zaitsev.ru.
2023-08-15 23:45:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroclinic.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:45:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroclinic.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:45:02 [root] ERROR: Parsing error
 url : https://zveroclinic.ru
details : DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:45:02 [root] ERROR: Parsing error
 url : https://zveroclinic.ru
details : DNS lookup failed: no results for hostname lookup: zveroclinic.ru.
2023-08-15 23:45:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodom.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverocomp.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:45:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveroff.ru> (referer: None)
2023-08-15 23:45:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverogorod.ru> (failed 1 times): 502 Bad Gateway
2023-08-15 23:45:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitceva.ru> (failed 3 times): User timeout caused connection failure: Getting https://zaitceva.ru took longer than 10.0 seconds..
2023-08-15 23:45:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zaitceva.ru> (failed 3 times): User timeout caused connection failure: Getting https://zaitceva.ru took longer than 10.0 seconds..
2023-08-15 23:45:03 [root] ERROR: Parsing error
 url : https://zaitceva.ru
details : User timeout caused connection failure: Getting https://zaitceva.ru took longer than 10.0 seconds..
2023-08-15 23:45:03 [root] ERROR: Parsing error
 url : https://zaitceva.ru
details : User timeout caused connection failure: Getting https://zaitceva.ru took longer than 10.0 seconds..
2023-08-15 23:45:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodrom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodrom.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:03 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodrom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverodrom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:03 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverodrom.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverodrom.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://zverodel.ru/> (referer: None)
2023-08-15 23:45:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverograd.ru/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverokon.ru/robots.txt> (referer: None)
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 112 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-08-15 23:45:03 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-08-15 23:45:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodar.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://zverojaschery.ru/robots.txt> (referer: None)
2023-08-15 23:45:05 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zverojaschery.ru>
2023-08-15 23:45:05 [root] ERROR: Parsing error
 url : https://zverojaschery.ru
details : Forbidden by robots.txt
2023-08-15 23:45:05 [root] ERROR: Parsing error
 url : https://zverojaschery.ru
details : Forbidden by robots.txt
2023-08-15 23:45:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://zverokorm.ru/robots.txt> (referer: None)
2023-08-15 23:45:05 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:45:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroklub.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:05 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zverokot.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zverokot.ru'))])
2023-08-15 23:45:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://zverokot.ru> from <GET https://zverokot.ru/robots.txt>
2023-08-15 23:45:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverokon.ru> (referer: None)
2023-08-15 23:45:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodom.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverogorod.ru> (failed 2 times): 502 Bad Gateway
2023-08-15 23:45:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverocomp.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:45:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverocomp.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:45:06 [root] ERROR: Parsing error
 url : https://zverocomp.ru
details : DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:45:06 [root] ERROR: Parsing error
 url : https://zverocomp.ru
details : DNS lookup failed: no results for hostname lookup: zverocomp.ru.
2023-08-15 23:45:07 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://zverokorm.ru> (referer: None)
2023-08-15 23:45:07 [root] ERROR: Parsing error
 url : https://zverokorm.ru
details : Ignoring non-200 response
2023-08-15 23:45:07 [root] ERROR: Parsing error
 url : https://zverokorm.ru
details : Ignoring non-200 response
2023-08-15 23:45:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodrom.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroklub.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverograd.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverograd.ru/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:08 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverograd.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:08 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverograd.ru/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodar.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:09 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://rf.ru/zverokot.ru> from <GET http://zverokot.ru>
2023-08-15 23:45:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolash.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverodrom.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:10 [root] ERROR: Parsing error
 url : https://zverodom.ru
details : DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:10 [root] ERROR: Parsing error
 url : https://zverodom.ru
details : DNS lookup failed: no results for hostname lookup: zverodom.ru.
2023-08-15 23:45:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroklub.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroklub.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroklub.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroklub.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverogorod.ru> (failed 3 times): 502 Bad Gateway
2023-08-15 23:45:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverogorod.ru> (failed 3 times): 502 Bad Gateway
2023-08-15 23:45:11 [scrapy.core.engine] DEBUG: Crawled (502) <GET https://zverogorod.ru> (referer: None)
2023-08-15 23:45:11 [root] ERROR: Parsing error
 url : https://zverogorod.ru
details : Ignoring non-200 response
2023-08-15 23:45:11 [root] ERROR: Parsing error
 url : https://zverogorod.ru
details : Ignoring non-200 response
2023-08-15 23:45:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverograd.ru> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolavka.ru/robots.txt> (referer: None)
2023-08-15 23:45:11 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zverolavka.ru>
2023-08-15 23:45:11 [root] ERROR: Parsing error
 url : https://zverolavka.ru
details : Forbidden by robots.txt
2023-08-15 23:45:11 [root] ERROR: Parsing error
 url : https://zverolavka.ru
details : Forbidden by robots.txt
2023-08-15 23:45:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolog.ru/robots.txt> (referer: None)
2023-08-15 23:45:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverok.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://zverok.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolov.ru/robots.txt> (referer: None)
2023-08-15 23:45:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodar.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodar.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:12 [root] ERROR: Parsing error
 url : https://zverodar.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:12 [root] ERROR: Parsing error
 url : https://zverodar.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolub.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverok.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolash.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroferma.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://zveroferma.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroferma.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://zveroferma.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroferma.ru/robots.txt>: User timeout caused connection failure: Getting https://zveroferma.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://zveroferma.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroferma.ru/robots.txt>: User timeout caused connection failure: Getting https://zveroferma.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://zveroferma.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rf.ru/zverokot.ru> (referer: None)
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 5 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-08-15 23:45:13 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-08-15 23:45:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolyub.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodrom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverodrom.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:14 [root] ERROR: Parsing error
 url : https://zverodrom.ru
details : DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:14 [root] ERROR: Parsing error
 url : https://zverodrom.ru
details : DNS lookup failed: no results for hostname lookup: zverodrom.ru.
2023-08-15 23:45:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroferma.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:45:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolog.ru> (referer: None)
2023-08-15 23:45:15 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "zveromag.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'zveromag.ru'))])
2023-08-15 23:45:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zveromag.ru/robots.txt> (referer: None)
2023-08-15 23:45:15 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://zveromag.ru>
2023-08-15 23:45:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroklub.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:15 [root] ERROR: Parsing error
 url : https://zveromag.ru
details : Forbidden by robots.txt
2023-08-15 23:45:15 [root] ERROR: Parsing error
 url : https://zveromag.ru
details : Forbidden by robots.txt
2023-08-15 23:45:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverok.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverok.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverok.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverok.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverok.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverok.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolub.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolov.ru> (referer: None)
2023-08-15 23:45:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverograd.ru> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolyub.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroland.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://zveroland.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroferma.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:45:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolash.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolash.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:17 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolash.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:17 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolash.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto> from <GET https://ecsto.ru/robots.txt>
2023-08-15 23:45:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://zverokot.ru> from <GET https://zverokot.ru>
2023-08-15 23:45:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveromania.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://zverolog.ru/kontakty/> (referer: https://zverolog.ru)
2023-08-15 23:45:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverok.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverograd.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverograd.ru> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:19 [root] ERROR: Parsing error
 url : https://zverograd.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:19 [root] ERROR: Parsing error
 url : https://zverograd.ru
details : [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'wrong version number')]>]
2023-08-15 23:45:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://ecstom-allon4.tmp.sinergium.ru/robots.txt> from <GET https://ecstom-allon4.ru/robots.txt>
2023-08-15 23:45:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroklub.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://rf.ru/zverokot.ru> from <GET http://zverokot.ru>
2023-08-15 23:45:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rf.ru/robots.txt> (referer: None)
2023-08-15 23:45:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolub.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolub.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolub.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverolub.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolub.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverolub.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolyub.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolyub.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolyub.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverolyub.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zverolyub.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: zverolyub.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroferma.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:45:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroferma.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:45:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstom.ru/robots.txt> (referer: None)
2023-08-15 23:45:21 [root] ERROR: Parsing error
 url : https://zveroferma.ru
details : DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:45:21 [root] ERROR: Parsing error
 url : https://zveroferma.ru
details : DNS lookup failed: no results for hostname lookup: zveroferma.ru.
2023-08-15 23:45:21 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://eacportal.com/sertifikat-tr-eaehs-ts/robots.txt> from <GET https://ecstr-certificate.ru/robots.txt>
2023-08-15 23:45:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolash.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:21 [scrapy.extensions.logstats] INFO: Crawled 160 pages (at 39 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:45:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveromania.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:22 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto> (referer: None)
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 109 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 111 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 119 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 137 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 159 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 161 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 165 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 167 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 169 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 171 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 173 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 176 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 177 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 185 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 189 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 201 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 204 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 208 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 210 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 211 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 225 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 226 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 232 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 233 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 236 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 238 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 239 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 243 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 244 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 245 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 247 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 252 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 256 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 267 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 270 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 276 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 278 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 279 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 288 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 290 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 293 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 294 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 296 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 300 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 302 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 312 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 321 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 326 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 329 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 331 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 334 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 335 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 340 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 341 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 344 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 345 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 349 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 351 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 354 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 357 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 360 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 363 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 366 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 369 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 370 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 371 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 372 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 389 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 402 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 406 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 414 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 415 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 417 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 418 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 419 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 420 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 421 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 422 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 424 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 426 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 433 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 434 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 435 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 436 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 437 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 438 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 439 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 440 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 441 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 442 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 443 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 444 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 445 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 446 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 450 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 453 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 454 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 455 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 458 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 461 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 466 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 473 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 474 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 475 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 478 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 485 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 487 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 490 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 493 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 496 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 499 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 504 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 514 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 517 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 520 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 521 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 524 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 527 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 530 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 531 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 534 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 535 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 536 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 539 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 542 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 545 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 549 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 550 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 552 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 555 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 560 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 565 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 566 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 567 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 568 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 569 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 571 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 574 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 575 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 579 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 580 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 581 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 582 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 583 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 584 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 585 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 588 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 589 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 592 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 593 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 595 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 596 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 597 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 598 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 601 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 602 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 603 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 604 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 605 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 606 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 607 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 611 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 613 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 616 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 618 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 622 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 623 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 625 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 627 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 631 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 632 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 633 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 634 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 635 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 636 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 637 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 642 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 643 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 644 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 645 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 648 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 651 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 654 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 655 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 658 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 659 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 660 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 663 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 666 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 670 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 671 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 672 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 674 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 676 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 679 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 681 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 682 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 685 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 686 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 689 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 690 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 691 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 692 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 695 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 699 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 700 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 701 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 704 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 705 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 706 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 707 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 708 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 710 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 713 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 714 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 715 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 716 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 717 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 718 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 719 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 722 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 724 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 725 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 726 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 727 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 728 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 729 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 730 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 736 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 737 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 738 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 739 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 740 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 741 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 745 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 746 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 753 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 765 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 768 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 769 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 770 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 771 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 782 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 788 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 801 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 803 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 837 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 848 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 849 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 852 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 853 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 856 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 857 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 858 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 883 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 886 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 905 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 906 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 907 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 908 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 925 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 969 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 987 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1024 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1071 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1080 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1136 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1140 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1148 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1154 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1155 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1158 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1159 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1169 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1183 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1190 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1193 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1197 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1210 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1253 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1265 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1295 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1335 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1343 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1399 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1403 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1411 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1412 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1415 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1416 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1419 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1438 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1441 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1460 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1461 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1462 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1463 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1468 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1495 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1516 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1531 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1548 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1576 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1577 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1621 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1637 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1638 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1672 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1673 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1718 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1719 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1728 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1786 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1787 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1792 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1795 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1805 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1814 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1818 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1829 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1830 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1833 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1834 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1843 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1846 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1890 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1891 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1893 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1894 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1896 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1897 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1920 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1937 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1940 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1943 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1967 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 1973 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 6107 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 6126 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 6181 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 6197 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 6259 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 6275 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 10413 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 10432 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11069 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11071 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11087 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11099 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11117 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11130 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11142 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11153 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11165 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11177 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11217 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11218 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11219 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11220 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11221 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11222 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11223 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11224 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11225 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11226 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11227 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11228 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11229 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11236 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11249 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11250 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11251 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11252 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11253 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11254 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11255 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11256 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11257 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11258 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11259 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11260 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11261 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11278 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11299 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11313 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11365 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11381 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11393 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11432 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11451 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11464 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11469 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11470 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11504 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11523 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11532 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11533 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11534 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11537 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11540 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11541 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11554 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11557 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11588 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11656 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11660 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11665 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11682 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11697 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11712 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11727 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11742 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11757 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11772 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11787 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11814 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11872 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11880 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11898 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11914 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11930 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11936 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11937 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11940 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11966 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 11991 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12000 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12001 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12002 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12007 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12011 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12012 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12016 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12017 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12018 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12019 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12020 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12024 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12028 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12029 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12030 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12031 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12033 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12034 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12035 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12036 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12037 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12041 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12042 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12043 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12044 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12046 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12047 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12048 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12049 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12050 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12051 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12052 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12053 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12057 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12058 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12059 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12060 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12061 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12062 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12063 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12064 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12065 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12066 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12067 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12071 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12072 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12073 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12074 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12075 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12076 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12077 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12078 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12079 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12080 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12081 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12084 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12085 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12088 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12089 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12092 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12093 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12097 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12099 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12101 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12102 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12103 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12104 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12105 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12106 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12107 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12108 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12109 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12110 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12111 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12125 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12137 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12148 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12160 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12173 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12177 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12229 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12247 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12252 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12265 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12283 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12287 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12289 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12296 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12300 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12313 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12316 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12319 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12330 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12331 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12332 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12333 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12334 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12335 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12336 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12337 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12338 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12340 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12341 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12342 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12351 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12352 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12353 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12354 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12388 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12426 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12427 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12428 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12429 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12480 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12516 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12529 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12571 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12595 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12598 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12599 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12600 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12601 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12612 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12613 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12631 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12660 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12671 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12713 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12714 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12724 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12725 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12728 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12739 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12804 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12806 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12814 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12821 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12851 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12866 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12867 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12868 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12875 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12911 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12947 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 12960 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13002 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13026 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13029 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13030 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13031 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13032 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13047 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13076 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13087 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13129 without any user agent to enforce it on.
2023-08-15 23:45:22 [protego] DEBUG: Rule at line 13130 without any user agent to enforce it on.
2023-08-15 23:45:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ecstom-allon4.tmp.sinergium.ru/robots.txt> (referer: None)
2023-08-15 23:45:23 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://ecstom-allon4.ru>
2023-08-15 23:45:23 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ecstract.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ecstract.ru'))])
2023-08-15 23:45:23 [root] ERROR: Parsing error
 url : https://ecstom-allon4.ru
details : Forbidden by robots.txt
2023-08-15 23:45:23 [root] ERROR: Parsing error
 url : https://ecstom-allon4.ru
details : Forbidden by robots.txt
2023-08-15 23:45:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://ecstract.ru/robots.txt> from <GET https://ecstract.ru/robots.txt>
2023-08-15 23:45:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverok.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroklub.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroklub.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:23 [root] ERROR: Parsing error
 url : https://zveroklub.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:23 [root] ERROR: Parsing error
 url : https://zveroklub.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstrm.ru/robots.txt> (referer: None)
2023-08-15 23:45:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rf.ru/zverokot.ru> (referer: None)
2023-08-15 23:45:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolub.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:24 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ecstro.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ecstro.ru'))])
2023-08-15 23:45:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://ecstro.ru/robots.txt> (referer: None)
2023-08-15 23:45:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 436: invalid continuation byte
2023-08-15 23:45:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolash.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://eacportal.com/sertifikat-tr-eaehs-ts/robots.txt> (referer: None)
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 59 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 218 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 225 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 251 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 258 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 263 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 264 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 266 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 267 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 284 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 326 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 336 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 424 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 432 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 530 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 531 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 542 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 565 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 572 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 580 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 590 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 623 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 644 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 645 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 660 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 662 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 666 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 700 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 761 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 770 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 782 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 792 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 803 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 814 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 815 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 821 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 822 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 824 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 825 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 827 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 828 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 830 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 831 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 845 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 846 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 847 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 848 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 849 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 851 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 852 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 853 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 855 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 856 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 857 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 859 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 860 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 862 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 863 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 864 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 865 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 867 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 868 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 869 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 871 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 872 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 873 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 874 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 875 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 876 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 877 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 878 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 879 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 880 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 881 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 882 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 883 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 884 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 885 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 886 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 887 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 888 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 889 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 890 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 891 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 892 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 893 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 894 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 895 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 896 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 897 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 898 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 899 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 900 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 901 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 902 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 903 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 904 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 911 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 944 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 962 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 973 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 976 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 981 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 982 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 983 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 989 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1002 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1005 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1006 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1007 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1008 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1009 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1012 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1014 without any user agent to enforce it on.
2023-08-15 23:45:24 [protego] DEBUG: Rule at line 1016 without any user agent to enforce it on.
2023-08-15 23:45:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolyub.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto> from <GET https://ecsto.ru>
2023-08-15 23:45:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.emcmos.ru/robots.txt> (referer: None)
2023-08-15 23:45:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstom.ru> (referer: None)
2023-08-15 23:45:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveromania.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveromania.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveromania.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveromania.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverok.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverok.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstro.ru> (referer: None)
2023-08-15 23:45:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroland.ru/robots.txt> (failed 2 times): User timeout caused connection failure: Getting https://zveroland.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:27 [root] ERROR: Parsing error
 url : https://zverok.ru
details : DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:27 [root] ERROR: Parsing error
 url : https://zverok.ru
details : DNS lookup failed: no results for hostname lookup: zverok.ru.
2023-08-15 23:45:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstudent.ru/robots.txt> (referer: None)
2023-08-15 23:45:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstroy.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://eacportal.com/sertifikat-tr-eaehs-ts/> from <GET https://ecstr-certificate.ru>
2023-08-15 23:45:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://eacportal.com/robots.txt> (referer: None)
2023-08-15 23:45:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ecstract.ru/robots.txt> (referer: None)
2023-08-15 23:45:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolub.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstrm.ru> (referer: None)
2023-08-15 23:45:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolash.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolash.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:28 [root] ERROR: Parsing error
 url : https://zverolash.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:28 [root] ERROR: Parsing error
 url : https://zverolash.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveromania.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstom.ru/contact-us/> (referer: https://ecstom.ru)
2023-08-15 23:45:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecstom.ru/contact-us/> (referer: https://ecstom.ru)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 100, in parse
    await csvwriter.fill_csv(response.url, title, description, emails, phones, postal_codes, inns, ogrns)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\CSVwriter.py", line 24, in fill_csv
    await writer.writerow((url, title, description, emails, phones, postal_codes, inns, ogrns))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\aiofiles\threadpool\utils.py", line 43, in method
    return await self._loop.run_in_executor(self._executor, cb)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\encodings\cp1251.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f9b7' in position 39: character maps to <undefined>
2023-08-15 23:45:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecstom.ru/contact-us/> (referer: https://ecstom.ru)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 100, in parse
    await csvwriter.fill_csv(response.url, title, description, emails, phones, postal_codes, inns, ogrns)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\CSVwriter.py", line 24, in fill_csv
    await writer.writerow((url, title, description, emails, phones, postal_codes, inns, ogrns))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\aiofiles\threadpool\utils.py", line 43, in method
    return await self._loop.run_in_executor(self._executor, cb)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\encodings\cp1251.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f9b7' in position 39: character maps to <undefined>
2023-08-15 23:45:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zverolyub.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstudent.ru> (referer: None)
2023-08-15 23:45:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstudio.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstro.ru/contacts.html> (referer: https://ecstro.ru)
2023-08-15 23:45:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto> (referer: None)
2023-08-15 23:45:30 [root] ERROR: Parsing error
 url : https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto
details : Ignoring non-200 response
2023-08-15 23:45:30 [root] ERROR: Parsing error
 url : https://www.emcmos.ru/clinics/evropeyskaya-klinika-sportivnoy-travmatologii-i-ortopedii-ecsto
details : Ignoring non-200 response
2023-08-15 23:45:30 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://ecstrm.ru/contact-us/> from <GET http://ecstrm.ru/contact-us/>
2023-08-15 23:45:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstroy.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolub.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolub.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:31 [root] ERROR: Parsing error
 url : https://zverolub.ru
details : DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:31 [root] ERROR: Parsing error
 url : https://zverolub.ru
details : DNS lookup failed: no results for hostname lookup: zverolub.ru.
2023-08-15 23:45:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsurgut.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://eacportal.com/sertifikat-tr-eaehs-ts/> (referer: None)
2023-08-15 23:45:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv24.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://eacportal.com/sertifikat-tr-eaehs-ts/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 100, in parse
    await csvwriter.fill_csv(response.url, title, description, emails, phones, postal_codes, inns, ogrns)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\CSVwriter.py", line 24, in fill_csv
    await writer.writerow((url, title, description, emails, phones, postal_codes, inns, ogrns))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\aiofiles\threadpool\utils.py", line 43, in method
    return await self._loop.run_in_executor(self._executor, cb)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\encodings\cp1251.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 106-107: character maps to <undefined>
2023-08-15 23:45:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://eacportal.com/sertifikat-tr-eaehs-ts/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 100, in parse
    await csvwriter.fill_csv(response.url, title, description, emails, phones, postal_codes, inns, ogrns)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\CSVwriter.py", line 24, in fill_csv
    await writer.writerow((url, title, description, emails, phones, postal_codes, inns, ogrns))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\aiofiles\threadpool\utils.py", line 43, in method
    return await self._loop.run_in_executor(self._executor, cb)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\usaid\AppData\Local\Programs\Python\Python310\lib\encodings\cp1251.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 106-107: character maps to <undefined>
2023-08-15 23:45:31 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://ecstract.ru/> from <GET https://ecstract.ru>
2023-08-15 23:45:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolyub.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zverolyub.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv24.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstudio.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:33 [root] ERROR: Parsing error
 url : https://zverolyub.ru
details : DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:33 [root] ERROR: Parsing error
 url : https://zverolyub.ru
details : DNS lookup failed: no results for hostname lookup: zverolyub.ru.
2023-08-15 23:45:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveromania.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecstrm.ru/contact-us/> (referer: None)
2023-08-15 23:45:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsurgut.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstroy.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstroy.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecstroy.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecstroy.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsys.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsy.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstudio.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstudio.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecstudio.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecstudio.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroland.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://zveroland.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroland.ru/robots.txt> (failed 3 times): User timeout caused connection failure: Getting https://zveroland.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroland.ru/robots.txt>: User timeout caused connection failure: Getting https://zveroland.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://zveroland.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://zveroland.ru/robots.txt>: User timeout caused connection failure: Getting https://zveroland.ru/robots.txt took longer than 10.0 seconds..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 397, in _cb_timeout
    raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://zveroland.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:37 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://ecsyst.ru/robots.txt> (referer: None)
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-08-15 23:45:37 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsurgut.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsurgut.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsurgut.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsurgut.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:37 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ecsystem.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ecsystem.ru'))])
2023-08-15 23:45:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://ecsystem.ru/robots.txt> (referer: None)
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv24.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv24.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsv24.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ecsv24.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsv24.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ecsv24.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveromania.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveromania.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:37 [root] ERROR: Parsing error
 url : https://zveromania.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:37 [root] ERROR: Parsing error
 url : https://zveromania.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ecstract.ru/> (referer: None)
2023-08-15 23:45:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecsytech.ru/robots.txt> (referer: None)
2023-08-15 23:45:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsy.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsys.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecsystem.ru> (referer: None)
2023-08-15 23:45:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:39 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsv.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:39 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsv.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstroy.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecszhbi.ru/robots.txt> (referer: None)
2023-08-15 23:45:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsystems.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstudio.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ecstract.ru/contacts.html> (referer: http://ecstract.ru/)
2023-08-15 23:45:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-auto.ru/robots.txt> (referer: None)
2023-08-15 23:45:41 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://ecsyst.ru> (referer: None)
2023-08-15 23:45:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsurgut.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:41 [root] ERROR: Parsing error
 url : https://ecsyst.ru
details : Ignoring non-200 response
2023-08-15 23:45:41 [root] ERROR: Parsing error
 url : https://ecsyst.ru
details : Ignoring non-200 response
2023-08-15 23:45:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-service.ru/robots.txt> (referer: None)
2023-08-15 23:45:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv24.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstroy.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsys.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsys.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:42 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsys.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ecsys.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:42 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsys.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ecsys.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecsytech.ru> (referer: None)
2023-08-15 23:45:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-shop.ru/robots.txt> (referer: None)
2023-08-15 23:45:42 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://ect-shop.ru>
2023-08-15 23:45:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecsytech.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdd in position 105: invalid continuation byte
2023-08-15 23:45:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://ecsytech.ru> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdd in position 105: invalid continuation byte
2023-08-15 23:45:43 [root] ERROR: Parsing error
 url : https://ect-shop.ru
details : Forbidden by robots.txt
2023-08-15 23:45:43 [root] ERROR: Parsing error
 url : https://ect-shop.ru
details : Forbidden by robots.txt
2023-08-15 23:45:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsy.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsy.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsy.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsy.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:43 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ect-travel.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ect-travel.ru'))])
2023-08-15 23:45:43 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://ect-travel.ru/robots.txt> from <GET https://ect-travel.ru/robots.txt>
2023-08-15 23:45:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsystems.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:43 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ect-telecoms.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ect-telecoms.ru'))])
2023-08-15 23:45:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-telecoms.ru/robots.txt> (referer: None)
2023-08-15 23:45:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecstudio.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecszhbi.ru> (referer: None)
2023-08-15 23:45:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-truck.ru/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:45:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-auto.ru> (referer: None)
2023-08-15 23:45:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsys.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsurgut.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv24.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-service.ru> (referer: None)
2023-08-15 23:45:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-telecoms.ru> (failed 1 times): 500 Internal Server Error
2023-08-15 23:45:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstroy.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstroy.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-truck.ru/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:45:46 [root] ERROR: Parsing error
 url : https://ecstroy.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:46 [root] ERROR: Parsing error
 url : https://ecstroy.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroland.ru> (failed 1 times): User timeout caused connection failure.
2023-08-15 23:45:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ect-travel.ru/robots.txt> (referer: None)
2023-08-15 23:45:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ecszhbi.ru/contacts/> (referer: https://ecszhbi.ru)
2023-08-15 23:45:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.ect.ru/robots.txt> from <GET https://ect.ru/robots.txt>
2023-08-15 23:45:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsy.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsystems.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsystems.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:47 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsystems.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:47 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecsystems.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsv.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstudio.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecstudio.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:48 [root] ERROR: Parsing error
 url : https://ecstudio.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:48 [root] ERROR: Parsing error
 url : https://ecstudio.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv24.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv24.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:48 [root] ERROR: Parsing error
 url : https://ecsv24.ru
details : DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:48 [root] ERROR: Parsing error
 url : https://ecsv24.ru
details : DNS lookup failed: no results for hostname lookup: ecsv24.ru.
2023-08-15 23:45:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-auto.ru/index.phtml?id=6> (referer: https://ect-auto.ru)
2023-08-15 23:45:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsys.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsurgut.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsurgut.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:49 [root] ERROR: Parsing error
 url : https://ecsurgut.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:49 [root] ERROR: Parsing error
 url : https://ecsurgut.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-truck.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:45:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-truck.ru/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:45:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ect-truck.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ect-truck.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:45:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ect-truck.ru/robots.txt>: DNS lookup failed: no results for hostname lookup: ect-truck.ru.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:45:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://ect77.ru/robots.txt> (referer: None)
2023-08-15 23:45:49 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-08-15 23:45:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect4e.ru/robots.txt> (referer: None)
2023-08-15 23:45:50 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ect-telecoms.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ect-telecoms.ru'))])
2023-08-15 23:45:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect-service.ru/ru/contact-us> (referer: https://ect-service.ru)
2023-08-15 23:45:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-telecoms.ru> (failed 2 times): 500 Internal Server Error
2023-08-15 23:45:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsy.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.ect.ru/robots.txt> (referer: None)
2023-08-15 23:45:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ectaco.ru/robots.txt> (referer: None)
2023-08-15 23:45:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://ect-travel.ru/> from <GET https://ect-travel.ru>
2023-08-15 23:45:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsystems.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsv.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:52 [root] ERROR: Parsing error
 url : https://ecsv.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:52 [root] ERROR: Parsing error
 url : https://ecsv.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecta.ru/robots.txt> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-truck.ru> (failed 1 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:45:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect77.ru> (referer: None)
2023-08-15 23:45:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect4e.ru> (referer: None)
2023-08-15 23:45:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsys.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsys.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:53 [root] ERROR: Parsing error
 url : https://ecsys.ru
details : DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:53 [root] ERROR: Parsing error
 url : https://ecsys.ru
details : DNS lookup failed: no results for hostname lookup: ecsys.ru.
2023-08-15 23:45:54 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "ect-telecoms.ru"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'ect-telecoms.ru'))])
2023-08-15 23:45:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-telecoms.ru> (failed 3 times): 500 Internal Server Error
2023-08-15 23:45:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-telecoms.ru> (failed 3 times): 500 Internal Server Error
2023-08-15 23:45:54 [scrapy.core.engine] DEBUG: Crawled (500) <GET https://ect-telecoms.ru> (referer: None)
2023-08-15 23:45:54 [root] ERROR: Parsing error
 url : https://ect-telecoms.ru
details : Ignoring non-200 response
2023-08-15 23:45:54 [root] ERROR: Parsing error
 url : https://ect-telecoms.ru
details : Ignoring non-200 response
2023-08-15 23:45:55 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.ect.ru/> from <GET https://ect.ru>
2023-08-15 23:45:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.ect.ru/robots.txt> (referer: None)
2023-08-15 23:45:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsy.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsy.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ect4e.ru/contacts/> (referer: https://ect4e.ru)
2023-08-15 23:45:55 [root] ERROR: Parsing error
 url : https://ecsy.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:55 [root] ERROR: Parsing error
 url : https://ecsy.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ectaco.ru> (referer: None)
2023-08-15 23:45:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecta.ru/robots.txt> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ect-travel.ru/> (referer: None)
2023-08-15 23:45:56 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ect-travel.ru/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 7162: invalid continuation byte
2023-08-15 23:45:56 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ect-travel.ru/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\defer.py", line 293, in aiter_errback
    yield await it.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\maininfo\maininfo\spiders\emails_spider.py", line 55, in parse
    decoded_body = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 7162: invalid continuation byte
2023-08-15 23:45:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecsystems.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://zveroland.ru> (failed 2 times): User timeout caused connection failure: Getting https://zveroland.ru took longer than 10.0 seconds..
2023-08-15 23:45:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect-truck.ru> (failed 2 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ectaco.ru/support/> (referer: https://ectaco.ru)
2023-08-15 23:45:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.ect.ru/> (referer: None)
2023-08-15 23:45:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecta.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecta.ru/robots.txt> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecta.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ecta.ru/robots.txt>: Connection was refused by other side: 10061:   , ..      ..
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect57.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://ect57.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:45:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsystems.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:45:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecsystems.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:00 [root] ERROR: Parsing error
 url : https://ecsystems.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:00 [root] ERROR: Parsing error
 url : https://ecsystems.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-truck.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:46:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect-truck.ru> (failed 3 times): DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:46:01 [root] ERROR: Parsing error
 url : https://ect-truck.ru
details : DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:46:01 [root] ERROR: Parsing error
 url : https://ect-truck.ru
details : DNS lookup failed: no results for hostname lookup: ect-truck.ru.
2023-08-15 23:46:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecta.ru> (failed 1 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ectastart.ru/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://ectastart.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:46:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ecta.ru> (failed 2 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroland.ru> (failed 3 times): User timeout caused connection failure.
2023-08-15 23:46:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://zveroland.ru> (failed 3 times): User timeout caused connection failure.
2023-08-15 23:46:07 [root] ERROR: Parsing error
 url : https://zveroland.ru
details : User timeout caused connection failure.
2023-08-15 23:46:07 [root] ERROR: Parsing error
 url : https://zveroland.ru
details : User timeout caused connection failure.
2023-08-15 23:46:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect57.ru/robots.txt> (failed 2 times): User timeout caused connection failure: Getting https://ect57.ru/robots.txt took longer than 10.0 seconds..
2023-08-15 23:46:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecta.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ecta.ru> (failed 3 times): Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:09 [root] ERROR: Parsing error
 url : https://ecta.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:09 [root] ERROR: Parsing error
 url : https://ecta.ru
details : Connection was refused by other side: 10061:   , ..      ..
2023-08-15 23:46:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ectastart.ru/robots.txt> (failed 2 times): User timeout caused connection failure.
2023-08-15 23:46:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect57.ru/robots.txt> (failed 3 times): User timeout caused connection failure.
2023-08-15 23:46:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect57.ru/robots.txt> (failed 3 times): User timeout caused connection failure.
2023-08-15 23:46:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ect57.ru/robots.txt>: User timeout caused connection failure.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2023-08-15 23:46:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ect57.ru/robots.txt>: User timeout caused connection failure.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2023-08-15 23:46:21 [scrapy.extensions.logstats] INFO: Crawled 212 pages (at 52 pages/min), scraped 0 items (at 0 items/min)
2023-08-15 23:46:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ectastart.ru/robots.txt> (failed 3 times): User timeout caused connection failure.
2023-08-15 23:46:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ectastart.ru/robots.txt> (failed 3 times): User timeout caused connection failure.
2023-08-15 23:46:22 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ectastart.ru/robots.txt>: User timeout caused connection failure.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2023-08-15 23:46:22 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://ectastart.ru/robots.txt>: User timeout caused connection failure.
Traceback (most recent call last):
  File "C:\Users\usaid\PycharmProjects\Scrapy_fp\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2023-08-15 23:46:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect57.ru> (failed 1 times): User timeout caused connection failure.
2023-08-15 23:46:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ectastart.ru> (failed 1 times): User timeout caused connection failure.
2023-08-15 23:46:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ect57.ru> (failed 2 times): User timeout caused connection failure.
2023-08-15 23:46:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://ectastart.ru> (failed 2 times): User timeout caused connection failure.
2023-08-15 23:46:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect57.ru> (failed 3 times): User timeout caused connection failure: Getting https://ect57.ru took longer than 10.0 seconds..
2023-08-15 23:46:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ect57.ru> (failed 3 times): User timeout caused connection failure: Getting https://ect57.ru took longer than 10.0 seconds..
2023-08-15 23:46:49 [root] ERROR: Parsing error
 url : https://ect57.ru
details : User timeout caused connection failure: Getting https://ect57.ru took longer than 10.0 seconds..
2023-08-15 23:46:49 [root] ERROR: Parsing error
 url : https://ect57.ru
details : User timeout caused connection failure: Getting https://ect57.ru took longer than 10.0 seconds..
2023-08-15 23:46:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ectastart.ru> (failed 3 times): User timeout caused connection failure: Getting https://ectastart.ru took longer than 10.0 seconds..
2023-08-15 23:46:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://ectastart.ru> (failed 3 times): User timeout caused connection failure: Getting https://ectastart.ru took longer than 10.0 seconds..
2023-08-15 23:46:52 [root] ERROR: Parsing error
 url : https://ectastart.ru
details : User timeout caused connection failure: Getting https://ectastart.ru took longer than 10.0 seconds..
2023-08-15 23:46:52 [root] ERROR: Parsing error
 url : https://ectastart.ru
details : User timeout caused connection failure: Getting https://ectastart.ru took longer than 10.0 seconds..
2023-08-15 23:46:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-08-15 23:46:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 591,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 15,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 150,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 322,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 44,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 60,
 'downloader/request_bytes': 248146,
 'downloader/request_count': 824,
 'downloader/request_method_count/GET': 824,
 'downloader/response_bytes': 3119568,
 'downloader/response_count': 248,
 'downloader/response_status_count/200': 184,
 'downloader/response_status_count/301': 12,
 'downloader/response_status_count/302': 14,
 'downloader/response_status_count/403': 7,
 'downloader/response_status_count/404': 16,
 'downloader/response_status_count/500': 6,
 'downloader/response_status_count/502': 3,
 'downloader/response_status_count/503': 6,
 'dupefilter/filtered': 27,
 'elapsed_time_seconds': 270.512579,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 8, 15, 20, 46, 52, 303517),
 'httpcompression/response_bytes': 16478904,
 'httpcompression/response_count': 162,
 'log_count/ERROR': 421,
 'request_depth_max': 2,
 'response_received_count': 212,
 'retry/count': 394,
 'retry/max_reached': 197,
 'retry/reason_count/500 Internal Server Error': 4,
 'retry/reason_count/502 Bad Gateway': 2,
 'retry/reason_count/503 Service Unavailable': 4,
 'retry/reason_count/twisted.internet.error.ConnectionRefusedError': 100,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 215,
 'retry/reason_count/twisted.internet.error.TimeoutError': 29,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 40,
 "robotstxt/exception_count/<class 'twisted.internet.error.ConnectionRefusedError'>": 25,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 53,
 "robotstxt/exception_count/<class 'twisted.internet.error.TimeoutError'>": 8,
 "robotstxt/exception_count/<class 'twisted.web._newclient.ResponseNeverReceived'>": 10,
 'robotstxt/forbidden': 15,
 'robotstxt/request_count': 199,
 'robotstxt/response_count': 103,
 'robotstxt/response_status_count/200': 85,
 'robotstxt/response_status_count/403': 2,
 'robotstxt/response_status_count/404': 15,
 'robotstxt/response_status_count/503': 1,
 'scheduler/dequeued': 433,
 'scheduler/dequeued/memory': 433,
 'scheduler/enqueued': 433,
 'scheduler/enqueued/memory': 433,
 'spider_exceptions/UnicodeDecodeError': 5,
 'spider_exceptions/UnicodeEncodeError': 2,
 'start_time': datetime.datetime(2023, 8, 15, 20, 42, 21, 790938)}
2023-08-15 23:46:52 [scrapy.core.engine] INFO: Spider closed (finished)
